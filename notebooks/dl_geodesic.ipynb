{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scprep\n","from tqdm import tqdm\n","import os\n","import sys\n","\n","sys.path.append('../src')\n","from models.unified_model import GeometricAE\n","from models.affinity_matching import AffinityMatching\n","from data_script import hemisphere_data, sklearn_swiss_roll\n","# from geodesic import CondCurve"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Data\n","gt_X, X, _ = hemisphere_data(n_samples=1000, noise=0)\n","#gt_X, X, colors = sklearn_swiss_roll(n_samples=2000, noise=0.0)\n","\n","\n","# Train, Test\n","percent_test = 0.2\n","idxs = np.random.permutation(X.shape[0])\n","split_idx = int(X.shape[0] * (1-percent_test))\n","train_mask = np.zeros(X.shape[0], dtype=int)\n","train_mask[idxs[:split_idx]] = 1\n","train_mask = train_mask.astype(bool)\n","\n","X_train = X[train_mask]\n","X_test = X[~train_mask]\n","\n","print(gt_X.shape, X.shape, X_train.shape, X_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize\n","scprep.plot.scatter3d(gt_X, c='b', title='Ground Truth')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit Affinity Matching Model\n","model_hypers = {\n","    'ambient_dimension': 3,\n","    'latent_dimension': 2,\n","    'model_type': 'affinity',\n","    'loss_type': 'kl',\n","    'activation': 'relu',\n","    'layer_widths': [256, 128, 64],\n","    'kernel_method': 'gaussian',\n","    'kernel_alpha': 1,\n","    'kernel_bandwidth': 1,\n","    'knn': 5,\n","    't': 0,\n","    'n_landmark': 5000,\n","    'verbose': False\n","}\n","training_hypers = {\n","    'data_name': 'hemisphere',\n","    'max_epochs': 100,\n","    'batch_size': 64,\n","    'lr': 1e-3,\n","    'shuffle': True,\n","    'weight_decay': 1e-5,\n","    'monitor': 'val_loss',\n","    'patience': 100,\n","    'seed': 2024,\n","    'log_every_n_steps': 100,\n","    'accelerator': 'auto',\n","    'train_from_scratch': True, # load or train from scratch\n","    'model_save_path': './affinity_matching'\n","}\n","\n","model = AffinityMatching(**model_hypers)\n","model.fit(X, train_mask=train_mask, percent_test=percent_test, **training_hypers)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Z = model.encode(torch.Tensor(X))\n","print('Encoded Z:', Z.shape)\n","X_hat = model.decode(Z)\n","print('Decoded X:', X_hat.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize\n","fig = plt.figure(figsize=(10, 5))\n","ax = fig.add_subplot(121, projection='3d')\n","scprep.plot.scatter3d(X_hat.detach().numpy(), c='r', title='Reconstructed', ax=ax)\n","ax = fig.add_subplot(122)\n","scprep.plot.scatter2d(Z.detach().numpy(), c='r', title='Latent', ax=ax)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metric = model.encoder_pullback(torch.Tensor(X))\n","print('Encoder Pullback:', metric.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # CondCurve\n","# class CondCurve(nn.Module):\n","#     def __init__(self, input_dim, hidden_dim, scale_factor=5, symmetric=False):\n","#         super().__init__()\n","#         self.mod_x0x1 = nn.Sequential(\n","#             nn.Linear((2 * hidden_dim) + 1, hidden_dim),\n","#             nn.ReLU(),\n","#             nn.Linear(hidden_dim, hidden_dim),\n","#             nn.ReLU(),\n","#             nn.Linear(hidden_dim, hidden_dim),\n","#             nn.ReLU(),\n","#             nn.Linear(hidden_dim, input_dim),\n","#         )\n","#         self.scale_factor = scale_factor\n","#         self.symmetric = symmetric\n","\n","#         if symmetric:\n","#             self.x0_x1_preemb = nn.Linear(2 * input_dim, 2 * hidden_dim)\n","#             self.x0_emb = nn.Linear(2 * hidden_dim, hidden_dim)\n","#             self.x1_emb = nn.Linear(2 * hidden_dim, hidden_dim)\n","#         else:\n","#             self.x0_emb = nn.Linear(input_dim, hidden_dim)\n","#             self.x1_emb = nn.Linear(input_dim, hidden_dim)\n","\n","#     def forward(self, x0, x1, t):\n","#         '''\n","#             x0, x1: [B, input_dim]\n","#             t: [T]\n","#         '''\n","#         bs = x0.shape[0]\n","#         T = t.shape[0]\n","#         if self.symmetric:\n","#             x0x1_emb = (\n","#                 self.x0_x1_preemb(torch.cat([x0, x1], dim=-1))\n","#                 + self.x0_x1_preemb(torch.cat([x1, x0], dim=-1))\n","#             ) * 0.5\n","#             emb_x0 = self.x0_emb(x0x1_emb)\n","#             emb_x1 = self.x1_emb(x0x1_emb)\n","#         else:\n","#             emb_x0 = self.x0_emb(x0)\n","#             emb_x1 = self.x1_emb(x1) # [B, hidden_dim]\n","#         t = t.view(-1, 1)\n","#         avg = t * x1 + (1 - t) * x0 # [B, T, input_dim]\n","#         print('avg.shape', avg.shape)\n","#         enveloppe = self.scale_factor * (1 - (t * 2 - 1) ** 2)\n","#         # Tile t to [B, T, 1]\n","#         t = torch.tile(t, (bs, 1, 1))\n","#         # Tile embx1,x0 to [B, T, input_dim]\n","#         emb_x0 = torch.tile(emb_x0, (1, T, 1))\n","#         emb_x1 = torch.tile(emb_x1, (1, T, 1))\n","#         print(emb_x0.shape, emb_x1.shape, t.shape)\n","\n","#         aug_state = torch.cat([emb_x0, emb_x1, t], dim=-1)\n","#         outs = self.mod_x0x1(aug_state) * enveloppe + avg\n","\n","#         return outs\n","    \n","#     # return a function that only takes t as input\n","#     def forward_rspt_t(self, x0, x1, t):\n","#         return lambda t: self.forward(x0, x1, t)\n","\n","# curve = CondCurve(input_dim=X.shape[1], hidden_dim=32, scale_factor=5, symmetric=False)\n","\n","# optimizer = torch.optim.Adam(curve.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CurveNet(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, x0, x1, variance = 0.15 ):\n","        super().__init__()\n","        self.mod = nn.Sequential(nn.Linear(1,hidden_dim),\n","                    nn.Tanh(),\n","                    nn.Linear(hidden_dim,hidden_dim),\n","                    nn.Tanh(),\n","                    nn.Linear(hidden_dim,hidden_dim),\n","                    nn.Tanh(),\n","                    nn.Linear(hidden_dim,input_dim + 1))\n","        self.x0 = x0\n","        self.x1 = x1\n","        self.variance = variance\n","        self.input_dim = input_dim\n","    def forward(self, t):\n","    \n","        mu = t * self.x1 + (1-t) * self.x0\n","        \n","        enveloppe = self.variance * (1- (t*2-1)**2) * self.mod(t)[:, self.input_dim].reshape(-1, 1)\n","        outs =  self.mod(t)[:,:self.input_dim] * enveloppe + mu\n","\n","        return outs\n","\n","# randomly pick a pair of points\n","batch_x0 = torch.Tensor(X_train[np.random.choice(X_train.shape[0], 1)])\n","batch_x1 = torch.Tensor(X_train[np.random.choice(X_train.shape[0], 1)])\n","\n","print('Batch X0:', batch_x0.shape)\n","print('Batch X1:', batch_x1.shape)\n","\n","curve = CurveNet(input_dim=X.shape[1], hidden_dim=32, x0=batch_x0, x1=batch_x1, variance=0.15)\n","optimizer = torch.optim.Adam(curve.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(batch_x0)\n","print(batch_x1)\n","\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","scprep.plot.scatter3d(X_train, c='b', title='Training Data', ax=ax)\n","scprep.plot.scatter3d(batch_x0, c='r', ax=ax)\n","scprep.plot.scatter3d(batch_x1, c='r', ax=ax)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["t = torch.linspace(0, 1, 100).view(-1,1)\n","losses = []\n","for _ in tqdm(range(500)):\n","\n","    optimizer.zero_grad()\n","\n","    #jac = torch.autograd.functional.jacobian(curve.forward, t, create_graph=True)\n","    jac = torch.autograd.functional.jacobian(curve.forward, t, create_graph=True)\n","    #print('jac: ', jac.shape)\n","    #print('jac = torch.einsum(\"tntd->tnd\",jac)', torch.einsum(\"tntd->tnd\",jac).shape)\n","    jac = torch.einsum(\"tntd->tnd\",jac)[...,0]\n","    #print('jac.shape', jac.shape)\n","\n","    out = curve(t)\n","        \n","    #m = metric(out)\n","    out_ = torch.Tensor(out.cpu().detach().numpy())\n","    m = model.encoder_pullback(out_)\n","\n","    pre_prod = torch.einsum('tb,tbj->tj',jac,m)\n","    prod = torch.einsum('tb,tb->t', pre_prod, jac)\n","\n","    loss = prod.mean()\n","    loss.backward()\n","    optimizer.step()\n","    losses.append(loss.detach().numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize\n","t = torch.linspace(0, 1, 1000).view(-1,1)\n","pred_geodesic = curve(t).detach().numpy()\n","print('Pred Geodesic:', pred_geodesic.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize pred geodesic on the latent space\n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","scprep.plot.scatter2d(Z.detach().numpy(), c='r', title='Latent', ax=ax)\n","# start and end points\n","scprep.plot.scatter2d(batch_x0, c='g', ax=ax)\n","scprep.plot.scatter2d(batch_x1, c='g', ax=ax)\n","scprep.plot.scatter2d(pred_geodesic, c='b', title='Pred Geodesic', ax=ax)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize pred geodesic on the ambient space\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","scprep.plot.scatter3d(X, c='r', title='Reconstructed', ax=ax)\n","# start and end points\n","scprep.plot.rotate_scatter3d(batch_x0, c='g', ax=ax)\n","scprep.plot.rotate_scatter3d(batch_x1, c='r', ax=ax)\n","scprep.plot.rotate_scatter3d(pred_geodesic, c='b', title='Pred Geodesic', ax=ax)"]}],"metadata":{"kernelspec":{"display_name":"another529","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":2}
