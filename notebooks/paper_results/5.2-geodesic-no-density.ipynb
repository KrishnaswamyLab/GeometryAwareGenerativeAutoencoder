{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxingzhis\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import scprep\n",
    "import pandas as pd\n",
    "sys.path.append('../../src/')\n",
    "from train import load_data\n",
    "# from diffusion import DiffusionModel\n",
    "# from evaluate import get_results\n",
    "from omegaconf import OmegaConf\n",
    "# from main import load_data, make_model\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import PillowWriter\n",
    "import torch\n",
    "from model2 import Autoencoder, Preprocessor, Discriminator\n",
    "import magic\n",
    "import torch\n",
    "import pathlib\n",
    "import copy\n",
    "\n",
    "import wandb\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import scprep\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from geodesic import jacobian, velocity, CondCurve, GeodesicBridgeOverfit\n",
    "from plotly3d.plot import scatter, trajectories\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from procrustes import Procrustes\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "wandb.login()\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distance_to_geodesic_criterion(predicted_geodesic, true_geodesic):\n",
    "    # the inputs here are single samples from a geodesic; should be shape num_samples x num_dims\n",
    "    # for each input point, we want the closest distance to any point on the true geodesic using the euclidean distance, torch.cdist\n",
    "    D = torch.cdist(predicted_geodesic, true_geodesic)\n",
    "    min_dists_to_true_geodesic = D.min(dim=1)[0]\n",
    "    # we take the mean of the squared distances\n",
    "    return torch.mean(min_dists_to_true_geodesic**2)\n",
    "# def distance_to_geodesic_criterion(\n",
    "#     predicted_geodesic:torch.Tensor, # size num_geodesics x num_samples x num_dims\n",
    "#     true_geodesic:torch.Tensor, # size num_geodesics num_samples x num_dims. But it's okay if the num_samples are different\n",
    "#     ):\n",
    "#     \"\"\"\n",
    "#     Mean of the squared distances from each predicted point to the closest point on the true geodesic\n",
    "#     \"\"\"\n",
    "#     dists = []\n",
    "#     for i in range(predicted_geodesic.shape[0]):\n",
    "#         dists.append(_distance_to_geodesic_criterion(predicted_geodesic[i], true_geodesic[i]))\n",
    "#     dists = torch.stack(dists)\n",
    "#     return dists.mean()\n",
    "\n",
    "def distance_to_geodesic_criterion_len(\n",
    "    predicted_geodesic:torch.Tensor, # size num_geodesics x num_samples x num_dims\n",
    "    true_geodesic:torch.Tensor, # size num_geodesics num_samples x num_dims. But it's okay if the num_samples are different\n",
    "    lengths=1.,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Mean of the squared distances from each predicted point to the closest point on the true geodesic\n",
    "    \"\"\"\n",
    "    dists = []\n",
    "    # for i in range(predicted_geodesic.shape[0]):\n",
    "    for i in range(len(predicted_geodesic)):\n",
    "        dists.append(_distance_to_geodesic_criterion(predicted_geodesic[i], true_geodesic[i]))\n",
    "    dists = torch.stack(dists)\n",
    "    dists = dists / lengths\n",
    "    return dists.mean()\n",
    "\n",
    "def distances_to_geodesic(\n",
    "    predicted_geodesic:torch.Tensor, # size num_geodesics x num_samples x num_dims\n",
    "    true_geodesic:torch.Tensor, # size num_geodesics num_samples x num_dims. But it's okay if the num_samples are different\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Mean of the squared distances from each predicted point to the closest point on the true geodesic\n",
    "    \"\"\"\n",
    "    dists = []\n",
    "    # for i in range(predicted_geodesic.shape[0]):\n",
    "    for i in range(len(predicted_geodesic)):\n",
    "        dists.append(_distance_to_geodesic_criterion(predicted_geodesic[i], true_geodesic[i]))\n",
    "    dists = torch.stack(dists)\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"xingzhis\"\n",
    "project = \"dmae\"\n",
    "# sweep_id = 'neh2vs3e'\n",
    "sweep_id = 'dcxgbhjp'\n",
    "sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "runs_data = []\n",
    "\n",
    "# Iterate through each run in the sweep\n",
    "for run in sweep.runs:\n",
    "    # Extract metrics and configs\n",
    "    metrics = run.summary._json_dict\n",
    "    configs = run.config\n",
    "    \n",
    "    # Combine metrics and configs, and add run ID\n",
    "    combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "    \n",
    "    # Append the combined data to the list\n",
    "    runs_data.append(combined_data)\n",
    "\n",
    "# Create a DataFrame from the runs data\n",
    "df = pd.DataFrame(runs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df[(df['loss_epoch']!='NaN')][['data_name']].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['loss_epoch']=='NaN')][['data_name']].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/pi/krishnaswamy_smita/xingzhi/.conda_envs/geosink/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'preprocessor' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['preprocessor'])`.\n",
      "  rank_zero_warn(\n",
      " 39%|███▉      | 14/36 [00:08<00:10,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) saddle_10_0.1\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 15/36 [00:08<00:08,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found 9     z5dzkrwu\n",
      "22    z5dzkrwu\n",
      "Name: run_id, dtype: object hemisphere_50_0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 20/36 [00:11<00:05,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 26/36 [00:14<00:03,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) hemisphere_15_0.5\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:19<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res_list = []\n",
    "missing = []\n",
    "failed = []\n",
    "\n",
    "for data_name in tqdm(names):\n",
    "    try:\n",
    "        entity = \"xingzhis\"\n",
    "        project = \"dmae\"\n",
    "        sweep_id = 'ys48kno0'\n",
    "        sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "\n",
    "        runs_data = []\n",
    "\n",
    "        # Iterate through each run in the sweep\n",
    "        for run in sweep.runs:\n",
    "            # Extract metrics and configs\n",
    "            metrics = run.summary._json_dict\n",
    "            configs = run.config\n",
    "            \n",
    "            # Combine metrics and configs, and add run ID\n",
    "            combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "            # Append the combined data to the list\n",
    "            runs_data.append(combined_data)\n",
    "\n",
    "        # Create a DataFrame from the runs data\n",
    "        df = pd.DataFrame(runs_data)\n",
    "\n",
    "        # run_ids = df[(df['data.name'] == data_name) & (df['cfg/loss/weights/cycle'] == 1.) & (df['cfg/dimensions/latent'] == 3)]['run_id']\n",
    "        run_ids = df[(df['data.name'] == data_name) & (df['loss.weights.cycle'] == 1.0) & (df['dimensions.latent'] == 3)]['run_id']\n",
    "        # assert len(run_ids) == 1\n",
    "        if len(run_ids) != 1:\n",
    "            print('AE not found:', run_ids, data_name)\n",
    "        run_id = run_ids.iloc[0]\n",
    "        run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        # run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        folder_path = '../../src/wandb/'\n",
    "        cfg = OmegaConf.create(run.config)\n",
    "        folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        ckpt_path = ckpt_files[0]\n",
    "        cfg.data.root = '../' + cfg.data.root\n",
    "        model = Autoencoder.load_from_checkpoint(ckpt_path)\n",
    "        data = np.load(f\"{cfg.data.root}/{cfg.data.name}{cfg.data.filetype}\", allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "        # sweep_id = 'ywep3ixr'\n",
    "        # sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "        # # Initialize an empty list to store run data\n",
    "        # runs_data = []\n",
    "\n",
    "        # # Iterate through each run in the sweep\n",
    "        # for run in sweep.runs:\n",
    "        #     # Extract metrics and configs\n",
    "        #     metrics = run.summary._json_dict\n",
    "        #     configs = run.config\n",
    "            \n",
    "        #     # Combine metrics and configs, and add run ID\n",
    "        #     combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "        #     # Append the combined data to the list\n",
    "        #     runs_data.append(combined_data)\n",
    "\n",
    "        # # Create a DataFrame from the runs data\n",
    "        # df = pd.DataFrame(runs_data)\n",
    "        # run_ids = df[(df['data.name'] == data_name)][['run_id']]\n",
    "        # assert len(run_ids) == 1\n",
    "        # run_id = run_ids.iloc[0]\n",
    "        # run = api.run(f\"{entity}/{project}/{run_ids.iloc[0].values[0]}\")\n",
    "        # folder_path = '../../src/wandb/'\n",
    "        # cfg = OmegaConf.create(run.config)\n",
    "        # folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        # ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        # ckpt_path = ckpt_files[0]\n",
    "        # cfg.data.root = '../' + cfg.data.root\n",
    "        # discriminator = Discriminator.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "\n",
    "\n",
    "        entity = \"xingzhis\"\n",
    "        project = \"dmae\"\n",
    "        sweep_id = 'neh2vs3e'\n",
    "        sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "        runs_data = []\n",
    "\n",
    "        # Iterate through each run in the sweep\n",
    "        for run in sweep.runs:\n",
    "            # Extract metrics and configs\n",
    "            metrics = run.summary._json_dict\n",
    "            configs = run.config\n",
    "            \n",
    "            # Combine metrics and configs, and add run ID\n",
    "            combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "            # Append the combined data to the list\n",
    "            runs_data.append(combined_data)\n",
    "\n",
    "        # Create a DataFrame from the runs data\n",
    "        df = pd.DataFrame(runs_data)\n",
    "\n",
    "        run_ids = df[(df['data_name'] == data_name) & (df['loss_epoch'] != 'NaN') & (df['dimensions_latent'] == 3)]['run_id']\n",
    "        # assert len(run_ids) == 1\n",
    "        if len(run_ids) != 1:\n",
    "            print('Geod not found', run_ids, data_name)\n",
    "        run_id = run_ids.iloc[0]\n",
    "        run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        cfg_main = OmegaConf.create(run.config)\n",
    "        folder_path = '../geodesic_on_datasets//wandb/'\n",
    "        folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        ckpt_path = ckpt_files[0]\n",
    "\n",
    "        x = torch.tensor(data['data'], dtype=torch.float32, device=model.device)\n",
    "        xbatch = torch.tensor(data['start_points'], dtype=x.dtype, device=x.device)\n",
    "        xendbatch = torch.tensor(data['end_points'], dtype=x.dtype, device=x.device)\n",
    "        # xbatch = model.encoder.preprocessor.normalize(xbatch)\n",
    "        # xendbatch = model.encoder.preprocessor.normalize(xendbatch)\n",
    "        # if cfg_main.overfit:\n",
    "        #     ids = torch.eye(xbatch.size(0))\n",
    "        # else:\n",
    "        ids = torch.zeros((xbatch.size(0),1))\n",
    "        dataset = TensorDataset(xbatch, xendbatch, ids)\n",
    "        dataloader = DataLoader(dataset, batch_size=len(x), shuffle=False)\n",
    "\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # def func(x):\n",
    "        #     return model.encoder(x)\n",
    "        # for param in discriminator.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # def discriminator_func_for_grad(x):\n",
    "        #     return discriminator.positive_proba(x, normalize=False).reshape(-1,1)\n",
    "        # def discriminator_func(x):\n",
    "        #     return discriminator.positive_proba(x, normalize=False).reshape(-1,1)\n",
    "        ofm = lambda x: x\n",
    "        gbmodel = GeodesicBridgeOverfit.load_from_checkpoint(\n",
    "            checkpoint_path=ckpt_path,\n",
    "            func=ofm,\n",
    "            # func = enc_func,\n",
    "            # discriminator_func=disc_func_pen,\n",
    "            # discriminator_func_for_grad=discriminator_func_for_grad,\n",
    "            input_dim=x.size(1), \n",
    "            hidden_dim=64, \n",
    "            scale_factor=1, \n",
    "            symmetric=True, \n",
    "            num_layers=3, \n",
    "            n_tsteps=100, \n",
    "            lr=1e-3, \n",
    "            weight_decay=1e-3,\n",
    "            discriminator_weight=0.,\n",
    "            discriminator_func_for_grad_weight=0.,\n",
    "            id_dim=1,\n",
    "            id_emb_dim=1,\n",
    "            density_weight=0.,\n",
    "            length_weight=1.,\n",
    "        )\n",
    "\n",
    "        # batch = next(iter(dataloader))\n",
    "        # x0, x1, ids = batch\n",
    "        try:\n",
    "            data_gt = np.load(f\"/gpfs/gibbs/pi/krishnaswamy_smita/xingzhi/dmae/data/neurips_results/toy/gt/{data_name}.npz\", allow_pickle=True)\n",
    "        except:\n",
    "            print(f\"CANNOT FIND FILE {data_name}.npz\")\n",
    "            missing.append(data_name)\n",
    "            continue\n",
    "        xbatch = torch.tensor(data_gt['start_points'], dtype=x.dtype, device=x.device)\n",
    "        xendbatch = torch.tensor(data_gt['end_points'], dtype=x.dtype, device=x.device)\n",
    "        x0 = xbatch\n",
    "        x1 = xendbatch\n",
    "        # xbatch = model.encoder.preprocessor.normalize(xbatch)\n",
    "        # xendbatch = model.encoder.preprocessor.normalize(xendbatch)\n",
    "        ids = torch.zeros((xbatch.size(0),1))\n",
    "        # ids = torch.eye((xbatch.size(0)))\n",
    "\n",
    "        # dataset = TensorDataset(xbatch, xendbatch, ids)\n",
    "        # dataloader = DataLoader(dataset, batch_size=len(z), shuffle=True)\n",
    "\n",
    "        def cc_func(x0, x1, t):\n",
    "            return gbmodel.cc(x0, x1, t, ids)\n",
    "        vectors = velocity(cc_func, gbmodel.ts, x0, x1)\n",
    "        cc_pts = gbmodel.cc(x0, x1, gbmodel.ts, ids)\n",
    "        vectors_flat = vectors.flatten(0,1)\n",
    "        cc_pts_flat = cc_pts.flatten(0, 1)\n",
    "        jac_flat = jacobian(gbmodel.func, cc_pts_flat)\n",
    "        length_all = torch.sqrt((torch.einsum(\"nij,nj->ni\", jac_flat, vectors_flat)**2).sum(axis=1))\n",
    "        length_all = length_all.reshape(vectors.shape[0], vectors.shape[1])\n",
    "        length = length_all.mean(axis=0)\n",
    "\n",
    "        geods = (cc_pts_flat).reshape(cc_pts.shape)\n",
    "        length2 = torch.sqrt(torch.diff(geods, axis=0)**2).sum(axis=-1).sum(axis=0)\n",
    "\n",
    "\n",
    "        # plt.scatter(length2.detach().numpy(), data_gt['geodesic_lengths'])\n",
    "        # plt.title(data_name)\n",
    "        # plt.show()\n",
    "\n",
    "        # gt_len = torch.tensor(data_gt['geodesic_lengths'])\n",
    "        true_geod = torch.tensor(data_gt['geodesics'], dtype=torch.float32)\n",
    "        gt_len = torch.sqrt(torch.diff(true_geod.permute(1,0,2), axis=0)**2).sum(axis=-1).sum(axis=0)\n",
    "        corr = np.corrcoef(length.detach().numpy(), gt_len.cpu().numpy())[0,1]\n",
    "        mse = ((length.detach().numpy() - gt_len.cpu().numpy())**2).mean()\n",
    "        dist2geod = distance_to_geodesic_criterion_len(geods.permute(1,0,2), true_geod, lengths=gt_len).detach().numpy()\n",
    "\n",
    "        res_list.append(dict(\n",
    "            name=data_name,\n",
    "            length_corr=corr,\n",
    "            length_mse=mse,\n",
    "            dist2geod=dist2geod\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        failed.append(data_name)\n",
    "        print(e)\n",
    "\n",
    "res_df = pd.DataFrame(res_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saddle_10_0.1', 'saddle_50_0.1', 'hemisphere_15_0.5']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) hemisphere_15_0.5\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res_list = []\n",
    "missing = []\n",
    "failed2 = []\n",
    "\n",
    "for data_name in tqdm(failed):\n",
    "    try:\n",
    "        entity = \"xingzhis\"\n",
    "        project = \"dmae\"\n",
    "        sweep_id = 'ys48kno0'\n",
    "        sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "\n",
    "        runs_data = []\n",
    "\n",
    "        # Iterate through each run in the sweep\n",
    "        for run in sweep.runs:\n",
    "            # Extract metrics and configs\n",
    "            metrics = run.summary._json_dict\n",
    "            configs = run.config\n",
    "            \n",
    "            # Combine metrics and configs, and add run ID\n",
    "            combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "            # Append the combined data to the list\n",
    "            runs_data.append(combined_data)\n",
    "\n",
    "        # Create a DataFrame from the runs data\n",
    "        df = pd.DataFrame(runs_data)\n",
    "\n",
    "        # run_ids = df[(df['data.name'] == data_name) & (df['cfg/loss/weights/cycle'] == 1.) & (df['cfg/dimensions/latent'] == 3)]['run_id']\n",
    "        run_ids = df[(df['data.name'] == data_name) & (df['loss.weights.cycle'] == 1.0) & (df['dimensions.latent'] == 3)]['run_id']\n",
    "        # assert len(run_ids) == 1\n",
    "        if len(run_ids) != 1:\n",
    "            print('AE not found:', run_ids, data_name)\n",
    "        run_id = run_ids.iloc[0]\n",
    "        run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        # run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        folder_path = '../../src/wandb/'\n",
    "        cfg = OmegaConf.create(run.config)\n",
    "        folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        ckpt_path = ckpt_files[0]\n",
    "        cfg.data.root = '../' + cfg.data.root\n",
    "        model = Autoencoder.load_from_checkpoint(ckpt_path)\n",
    "        data = np.load(f\"{cfg.data.root}/{cfg.data.name}{cfg.data.filetype}\", allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "        # sweep_id = 'ywep3ixr'\n",
    "        # sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "        # # Initialize an empty list to store run data\n",
    "        # runs_data = []\n",
    "\n",
    "        # # Iterate through each run in the sweep\n",
    "        # for run in sweep.runs:\n",
    "        #     # Extract metrics and configs\n",
    "        #     metrics = run.summary._json_dict\n",
    "        #     configs = run.config\n",
    "            \n",
    "        #     # Combine metrics and configs, and add run ID\n",
    "        #     combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "        #     # Append the combined data to the list\n",
    "        #     runs_data.append(combined_data)\n",
    "\n",
    "        # # Create a DataFrame from the runs data\n",
    "        # df = pd.DataFrame(runs_data)\n",
    "        # run_ids = df[(df['data.name'] == data_name)][['run_id']]\n",
    "        # assert len(run_ids) == 1\n",
    "        # run_id = run_ids.iloc[0]\n",
    "        # run = api.run(f\"{entity}/{project}/{run_ids.iloc[0].values[0]}\")\n",
    "        # folder_path = '../../src/wandb/'\n",
    "        # cfg = OmegaConf.create(run.config)\n",
    "        # folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        # ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        # ckpt_path = ckpt_files[0]\n",
    "        # cfg.data.root = '../' + cfg.data.root\n",
    "        # discriminator = Discriminator.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "\n",
    "\n",
    "        entity = \"xingzhis\"\n",
    "        project = \"dmae\"\n",
    "        sweep_id = '1b2p375i'\n",
    "        sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "        runs_data = []\n",
    "\n",
    "        # Iterate through each run in the sweep\n",
    "        for run in sweep.runs:\n",
    "            # Extract metrics and configs\n",
    "            metrics = run.summary._json_dict\n",
    "            configs = run.config\n",
    "            \n",
    "            # Combine metrics and configs, and add run ID\n",
    "            combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "            # Append the combined data to the list\n",
    "            runs_data.append(combined_data)\n",
    "\n",
    "        # Create a DataFrame from the runs data\n",
    "        df = pd.DataFrame(runs_data)\n",
    "\n",
    "        run_ids = df[(df['data_name'] == data_name) & (df['loss_epoch'] != 'NaN') & (df['dimensions_latent'] == 3)]['run_id']\n",
    "        # assert len(run_ids) == 1\n",
    "        if len(run_ids) != 1:\n",
    "            print('Geod not found', run_ids, data_name)\n",
    "        run_id = run_ids.iloc[0]\n",
    "        run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        cfg_main = OmegaConf.create(run.config)\n",
    "        folder_path = '../geodesic_on_datasets//wandb/'\n",
    "        folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        ckpt_path = ckpt_files[0]\n",
    "\n",
    "        x = torch.tensor(data['data'], dtype=torch.float32, device=model.device)\n",
    "        xbatch = torch.tensor(data['start_points'], dtype=x.dtype, device=x.device)\n",
    "        xendbatch = torch.tensor(data['end_points'], dtype=x.dtype, device=x.device)\n",
    "        # xbatch = model.encoder.preprocessor.normalize(xbatch)\n",
    "        # xendbatch = model.encoder.preprocessor.normalize(xendbatch)\n",
    "        # if cfg_main.overfit:\n",
    "        #     ids = torch.eye(xbatch.size(0))\n",
    "        # else:\n",
    "        ids = torch.zeros((xbatch.size(0),1))\n",
    "        dataset = TensorDataset(xbatch, xendbatch, ids)\n",
    "        dataloader = DataLoader(dataset, batch_size=len(x), shuffle=False)\n",
    "\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # def func(x):\n",
    "        #     return model.encoder(x)\n",
    "        # for param in discriminator.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # def discriminator_func_for_grad(x):\n",
    "        #     return discriminator.positive_proba(x, normalize=False).reshape(-1,1)\n",
    "        # def discriminator_func(x):\n",
    "        #     return discriminator.positive_proba(x, normalize=False).reshape(-1,1)\n",
    "        ofm = lambda x: x\n",
    "        gbmodel = GeodesicBridgeOverfit.load_from_checkpoint(\n",
    "            checkpoint_path=ckpt_path,\n",
    "            func=ofm,\n",
    "            # func = enc_func,\n",
    "            # discriminator_func=disc_func_pen,\n",
    "            # discriminator_func_for_grad=discriminator_func_for_grad,\n",
    "            input_dim=x.size(1), \n",
    "            hidden_dim=64, \n",
    "            scale_factor=1, \n",
    "            symmetric=True, \n",
    "            num_layers=3, \n",
    "            n_tsteps=100, \n",
    "            lr=1e-3, \n",
    "            weight_decay=1e-3,\n",
    "            discriminator_weight=0.,\n",
    "            discriminator_func_for_grad_weight=0.,\n",
    "            id_dim=1,\n",
    "            id_emb_dim=1,\n",
    "            density_weight=0.,\n",
    "            length_weight=1.,\n",
    "        )\n",
    "\n",
    "        # batch = next(iter(dataloader))\n",
    "        # x0, x1, ids = batch\n",
    "        try:\n",
    "            data_gt = np.load(f\"/gpfs/gibbs/pi/krishnaswamy_smita/xingzhi/dmae/data/neurips_results/toy/gt/{data_name}.npz\", allow_pickle=True)\n",
    "        except:\n",
    "            print(f\"CANNOT FIND FILE {data_name}.npz\")\n",
    "            missing.append(data_name)\n",
    "            continue\n",
    "        xbatch = torch.tensor(data_gt['start_points'], dtype=x.dtype, device=x.device)\n",
    "        xendbatch = torch.tensor(data_gt['end_points'], dtype=x.dtype, device=x.device)\n",
    "        x0 = xbatch\n",
    "        x1 = xendbatch\n",
    "        # xbatch = model.encoder.preprocessor.normalize(xbatch)\n",
    "        # xendbatch = model.encoder.preprocessor.normalize(xendbatch)\n",
    "        ids = torch.zeros((xbatch.size(0),1))\n",
    "        # ids = torch.eye((xbatch.size(0)))\n",
    "\n",
    "        # dataset = TensorDataset(xbatch, xendbatch, ids)\n",
    "        # dataloader = DataLoader(dataset, batch_size=len(z), shuffle=True)\n",
    "\n",
    "        def cc_func(x0, x1, t):\n",
    "            return gbmodel.cc(x0, x1, t, ids)\n",
    "        vectors = velocity(cc_func, gbmodel.ts, x0, x1)\n",
    "        cc_pts = gbmodel.cc(x0, x1, gbmodel.ts, ids)\n",
    "        vectors_flat = vectors.flatten(0,1)\n",
    "        cc_pts_flat = cc_pts.flatten(0, 1)\n",
    "        jac_flat = jacobian(gbmodel.func, cc_pts_flat)\n",
    "        length_all = torch.sqrt((torch.einsum(\"nij,nj->ni\", jac_flat, vectors_flat)**2).sum(axis=1))\n",
    "        length_all = length_all.reshape(vectors.shape[0], vectors.shape[1])\n",
    "        length = length_all.mean(axis=0)\n",
    "\n",
    "        geods = (cc_pts_flat).reshape(cc_pts.shape)\n",
    "        length2 = torch.sqrt(torch.diff(geods, axis=0)**2).sum(axis=-1).sum(axis=0)\n",
    "\n",
    "\n",
    "        # plt.scatter(length2.detach().numpy(), data_gt['geodesic_lengths'])\n",
    "        # plt.title(data_name)\n",
    "        # plt.show()\n",
    "\n",
    "        # gt_len = torch.tensor(data_gt['geodesic_lengths'])\n",
    "        true_geod = torch.tensor(data_gt['geodesics'], dtype=torch.float32)\n",
    "        gt_len = torch.sqrt(torch.diff(true_geod.permute(1,0,2), axis=0)**2).sum(axis=-1).sum(axis=0)\n",
    "        corr = np.corrcoef(length.detach().numpy(), gt_len.cpu().numpy())[0,1]\n",
    "        mse = ((length.detach().numpy() - gt_len.cpu().numpy())**2).mean()\n",
    "        dist2geod = distance_to_geodesic_criterion_len(geods.permute(1,0,2), true_geod, lengths=gt_len).detach().numpy()\n",
    "\n",
    "        res_list.append(dict(\n",
    "            name=data_name,\n",
    "            length_corr=corr,\n",
    "            length_mse=mse,\n",
    "            dist2geod=dist2geod\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        failed2.append(data_name)\n",
    "        print(e)\n",
    "\n",
    "res_df2 = pd.DataFrame(res_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ours = pd.concat([res_df, res_df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>length_corr</th>\n",
       "      <th>length_mse</th>\n",
       "      <th>dist2geod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ellipsoid_10_0.1</td>\n",
       "      <td>0.468024</td>\n",
       "      <td>6.542765</td>\n",
       "      <td>0.14057913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ellipsoid_15_0</td>\n",
       "      <td>0.913999</td>\n",
       "      <td>13.506490</td>\n",
       "      <td>0.024221402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ellipsoid_15_0.1</td>\n",
       "      <td>0.845484</td>\n",
       "      <td>19.020750</td>\n",
       "      <td>0.05549545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ellipsoid_15_0.3</td>\n",
       "      <td>0.610557</td>\n",
       "      <td>14.293184</td>\n",
       "      <td>0.30313063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ellipsoid_15_0.5</td>\n",
       "      <td>0.556689</td>\n",
       "      <td>12.079306</td>\n",
       "      <td>0.56921166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ellipsoid_15_0.7</td>\n",
       "      <td>-0.073554</td>\n",
       "      <td>12.685026</td>\n",
       "      <td>1.1486657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ellipsoid_50_0.1</td>\n",
       "      <td>0.629106</td>\n",
       "      <td>25.590904</td>\n",
       "      <td>0.13367188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ellipsoid_5_0.1</td>\n",
       "      <td>0.501222</td>\n",
       "      <td>9.889479</td>\n",
       "      <td>0.059939913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ellipsoid_none_0.1</td>\n",
       "      <td>0.273180</td>\n",
       "      <td>5.330163</td>\n",
       "      <td>0.1547515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hemisphere_10_0.1</td>\n",
       "      <td>0.976843</td>\n",
       "      <td>2.880428</td>\n",
       "      <td>0.048764646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hemisphere_15_0</td>\n",
       "      <td>0.979294</td>\n",
       "      <td>3.594204</td>\n",
       "      <td>0.013288553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hemisphere_15_0.1</td>\n",
       "      <td>0.564702</td>\n",
       "      <td>4.051459</td>\n",
       "      <td>0.08095882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hemisphere_15_0.3</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>2.475684</td>\n",
       "      <td>0.7199272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hemisphere_15_0.7</td>\n",
       "      <td>0.162641</td>\n",
       "      <td>3.684200</td>\n",
       "      <td>2.1859736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hemisphere_50_0.1</td>\n",
       "      <td>0.865310</td>\n",
       "      <td>4.576982</td>\n",
       "      <td>0.20637679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hemisphere_5_0.1</td>\n",
       "      <td>0.789452</td>\n",
       "      <td>0.623651</td>\n",
       "      <td>0.051760465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>hemisphere_none_0.1</td>\n",
       "      <td>0.918701</td>\n",
       "      <td>0.422754</td>\n",
       "      <td>0.04186386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>saddle_10_0.1</td>\n",
       "      <td>0.882665</td>\n",
       "      <td>2.007844</td>\n",
       "      <td>0.08494645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>saddle_15_0</td>\n",
       "      <td>0.973810</td>\n",
       "      <td>4.968450</td>\n",
       "      <td>0.015077899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>saddle_15_0.1</td>\n",
       "      <td>0.869598</td>\n",
       "      <td>3.940537</td>\n",
       "      <td>0.050379395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>saddle_15_0.3</td>\n",
       "      <td>0.677027</td>\n",
       "      <td>1.541096</td>\n",
       "      <td>0.34323764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>saddle_15_0.5</td>\n",
       "      <td>0.395536</td>\n",
       "      <td>1.131228</td>\n",
       "      <td>0.88229084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>saddle_15_0.7</td>\n",
       "      <td>0.254303</td>\n",
       "      <td>3.995860</td>\n",
       "      <td>1.7504785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>saddle_50_0.1</td>\n",
       "      <td>0.939317</td>\n",
       "      <td>5.284430</td>\n",
       "      <td>0.13991937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>saddle_5_0.1</td>\n",
       "      <td>0.785498</td>\n",
       "      <td>1.382261</td>\n",
       "      <td>0.029197698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>saddle_none_0.1</td>\n",
       "      <td>0.933620</td>\n",
       "      <td>0.784032</td>\n",
       "      <td>0.018535296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>torus_10_0.1</td>\n",
       "      <td>0.940233</td>\n",
       "      <td>15.396060</td>\n",
       "      <td>0.065491825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>torus_15_0</td>\n",
       "      <td>0.982107</td>\n",
       "      <td>24.646395</td>\n",
       "      <td>0.03352942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>torus_15_0.1</td>\n",
       "      <td>0.982191</td>\n",
       "      <td>28.077686</td>\n",
       "      <td>0.05876852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>torus_15_0.3</td>\n",
       "      <td>0.903769</td>\n",
       "      <td>22.883854</td>\n",
       "      <td>0.22196908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>torus_15_0.5</td>\n",
       "      <td>0.866086</td>\n",
       "      <td>18.514904</td>\n",
       "      <td>0.50089777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>torus_15_0.7</td>\n",
       "      <td>0.522471</td>\n",
       "      <td>15.566595</td>\n",
       "      <td>0.95686424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>torus_50_0.1</td>\n",
       "      <td>0.925115</td>\n",
       "      <td>33.913094</td>\n",
       "      <td>0.10959925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>torus_5_0.1</td>\n",
       "      <td>0.904453</td>\n",
       "      <td>9.136180</td>\n",
       "      <td>0.05429554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>torus_none_0.1</td>\n",
       "      <td>0.835447</td>\n",
       "      <td>3.580664</td>\n",
       "      <td>0.190402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  length_corr  length_mse    dist2geod\n",
       "23     ellipsoid_10_0.1     0.468024    6.542765   0.14057913\n",
       "31       ellipsoid_15_0     0.913999   13.506490  0.024221402\n",
       "30     ellipsoid_15_0.1     0.845484   19.020750   0.05549545\n",
       "32     ellipsoid_15_0.3     0.610557   14.293184   0.30313063\n",
       "22     ellipsoid_15_0.5     0.556689   12.079306   0.56921166\n",
       "25     ellipsoid_15_0.7    -0.073554   12.685026    1.1486657\n",
       "29     ellipsoid_50_0.1     0.629106   25.590904   0.13367188\n",
       "24      ellipsoid_5_0.1     0.501222    9.889479  0.059939913\n",
       "26   ellipsoid_none_0.1     0.273180    5.330163    0.1547515\n",
       "18    hemisphere_10_0.1     0.976843    2.880428  0.048764646\n",
       "21      hemisphere_15_0     0.979294    3.594204  0.013288553\n",
       "27    hemisphere_15_0.1     0.564702    4.051459   0.08095882\n",
       "19    hemisphere_15_0.3     0.003218    2.475684    0.7199272\n",
       "20    hemisphere_15_0.7     0.162641    3.684200    2.1859736\n",
       "14    hemisphere_50_0.1     0.865310    4.576982   0.20637679\n",
       "13     hemisphere_5_0.1     0.789452    0.623651  0.051760465\n",
       "28  hemisphere_none_0.1     0.918701    0.422754   0.04186386\n",
       "33        saddle_10_0.1     0.882665    2.007844   0.08494645\n",
       "10          saddle_15_0     0.973810    4.968450  0.015077899\n",
       "8         saddle_15_0.1     0.869598    3.940537  0.050379395\n",
       "16        saddle_15_0.3     0.677027    1.541096   0.34323764\n",
       "12        saddle_15_0.5     0.395536    1.131228   0.88229084\n",
       "9         saddle_15_0.7     0.254303    3.995860    1.7504785\n",
       "34        saddle_50_0.1     0.939317    5.284430   0.13991937\n",
       "17         saddle_5_0.1     0.785498    1.382261  0.029197698\n",
       "15      saddle_none_0.1     0.933620    0.784032  0.018535296\n",
       "7          torus_10_0.1     0.940233   15.396060  0.065491825\n",
       "11           torus_15_0     0.982107   24.646395   0.03352942\n",
       "3          torus_15_0.1     0.982191   28.077686   0.05876852\n",
       "5          torus_15_0.3     0.903769   22.883854   0.22196908\n",
       "6          torus_15_0.5     0.866086   18.514904   0.50089777\n",
       "2          torus_15_0.7     0.522471   15.566595   0.95686424\n",
       "4          torus_50_0.1     0.925115   33.913094   0.10959925\n",
       "0           torus_5_0.1     0.904453    9.136180   0.05429554\n",
       "1        torus_none_0.1     0.835447    3.580664     0.190402"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ours.sort_values('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>length_corr</th>\n",
       "      <th>length_mse</th>\n",
       "      <th>dist2geod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>torus_5_0.1</td>\n",
       "      <td>0.904453</td>\n",
       "      <td>9.136180</td>\n",
       "      <td>0.05429554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>torus_none_0.1</td>\n",
       "      <td>0.835447</td>\n",
       "      <td>3.580664</td>\n",
       "      <td>0.190402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>torus_15_0.7</td>\n",
       "      <td>0.522471</td>\n",
       "      <td>15.566595</td>\n",
       "      <td>0.95686424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>torus_15_0.1</td>\n",
       "      <td>0.982191</td>\n",
       "      <td>28.077686</td>\n",
       "      <td>0.05876852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>torus_50_0.1</td>\n",
       "      <td>0.925115</td>\n",
       "      <td>33.913094</td>\n",
       "      <td>0.10959925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>torus_15_0.3</td>\n",
       "      <td>0.903769</td>\n",
       "      <td>22.883854</td>\n",
       "      <td>0.22196908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>torus_15_0.5</td>\n",
       "      <td>0.866086</td>\n",
       "      <td>18.514904</td>\n",
       "      <td>0.50089777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>torus_10_0.1</td>\n",
       "      <td>0.940233</td>\n",
       "      <td>15.396060</td>\n",
       "      <td>0.065491825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>saddle_15_0.1</td>\n",
       "      <td>0.869598</td>\n",
       "      <td>3.940537</td>\n",
       "      <td>0.050379395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>saddle_15_0.7</td>\n",
       "      <td>0.254303</td>\n",
       "      <td>3.995860</td>\n",
       "      <td>1.7504785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>saddle_15_0</td>\n",
       "      <td>0.973810</td>\n",
       "      <td>4.968450</td>\n",
       "      <td>0.015077899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>torus_15_0</td>\n",
       "      <td>0.982107</td>\n",
       "      <td>24.646395</td>\n",
       "      <td>0.03352942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>saddle_15_0.5</td>\n",
       "      <td>0.395536</td>\n",
       "      <td>1.131228</td>\n",
       "      <td>0.88229084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hemisphere_5_0.1</td>\n",
       "      <td>0.789452</td>\n",
       "      <td>0.623651</td>\n",
       "      <td>0.051760465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hemisphere_50_0.1</td>\n",
       "      <td>0.865310</td>\n",
       "      <td>4.576982</td>\n",
       "      <td>0.20637679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>saddle_none_0.1</td>\n",
       "      <td>0.933620</td>\n",
       "      <td>0.784032</td>\n",
       "      <td>0.018535296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>saddle_15_0.3</td>\n",
       "      <td>0.677027</td>\n",
       "      <td>1.541096</td>\n",
       "      <td>0.34323764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>saddle_5_0.1</td>\n",
       "      <td>0.785498</td>\n",
       "      <td>1.382261</td>\n",
       "      <td>0.029197698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hemisphere_10_0.1</td>\n",
       "      <td>0.976843</td>\n",
       "      <td>2.880428</td>\n",
       "      <td>0.048764646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hemisphere_15_0.3</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>2.475684</td>\n",
       "      <td>0.7199272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hemisphere_15_0.7</td>\n",
       "      <td>0.162641</td>\n",
       "      <td>3.684200</td>\n",
       "      <td>2.1859736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hemisphere_15_0</td>\n",
       "      <td>0.979294</td>\n",
       "      <td>3.594204</td>\n",
       "      <td>0.013288553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ellipsoid_15_0.5</td>\n",
       "      <td>0.556689</td>\n",
       "      <td>12.079306</td>\n",
       "      <td>0.56921166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ellipsoid_10_0.1</td>\n",
       "      <td>0.468024</td>\n",
       "      <td>6.542765</td>\n",
       "      <td>0.14057913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ellipsoid_5_0.1</td>\n",
       "      <td>0.501222</td>\n",
       "      <td>9.889479</td>\n",
       "      <td>0.059939913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ellipsoid_15_0.7</td>\n",
       "      <td>-0.073554</td>\n",
       "      <td>12.685026</td>\n",
       "      <td>1.1486657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ellipsoid_none_0.1</td>\n",
       "      <td>0.273180</td>\n",
       "      <td>5.330163</td>\n",
       "      <td>0.1547515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hemisphere_15_0.1</td>\n",
       "      <td>0.564702</td>\n",
       "      <td>4.051459</td>\n",
       "      <td>0.08095882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>hemisphere_none_0.1</td>\n",
       "      <td>0.918701</td>\n",
       "      <td>0.422754</td>\n",
       "      <td>0.04186386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ellipsoid_50_0.1</td>\n",
       "      <td>0.629106</td>\n",
       "      <td>25.590904</td>\n",
       "      <td>0.13367188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ellipsoid_15_0.1</td>\n",
       "      <td>0.845484</td>\n",
       "      <td>19.020750</td>\n",
       "      <td>0.05549545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ellipsoid_15_0</td>\n",
       "      <td>0.913999</td>\n",
       "      <td>13.506490</td>\n",
       "      <td>0.024221402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ellipsoid_15_0.3</td>\n",
       "      <td>0.610557</td>\n",
       "      <td>14.293184</td>\n",
       "      <td>0.30313063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>saddle_10_0.1</td>\n",
       "      <td>0.882665</td>\n",
       "      <td>2.007844</td>\n",
       "      <td>0.08494645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>saddle_50_0.1</td>\n",
       "      <td>0.939317</td>\n",
       "      <td>5.284430</td>\n",
       "      <td>0.13991937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  length_corr  length_mse    dist2geod\n",
       "0           torus_5_0.1     0.904453    9.136180   0.05429554\n",
       "1        torus_none_0.1     0.835447    3.580664     0.190402\n",
       "2          torus_15_0.7     0.522471   15.566595   0.95686424\n",
       "3          torus_15_0.1     0.982191   28.077686   0.05876852\n",
       "4          torus_50_0.1     0.925115   33.913094   0.10959925\n",
       "5          torus_15_0.3     0.903769   22.883854   0.22196908\n",
       "6          torus_15_0.5     0.866086   18.514904   0.50089777\n",
       "7          torus_10_0.1     0.940233   15.396060  0.065491825\n",
       "8         saddle_15_0.1     0.869598    3.940537  0.050379395\n",
       "9         saddle_15_0.7     0.254303    3.995860    1.7504785\n",
       "10          saddle_15_0     0.973810    4.968450  0.015077899\n",
       "11           torus_15_0     0.982107   24.646395   0.03352942\n",
       "12        saddle_15_0.5     0.395536    1.131228   0.88229084\n",
       "13     hemisphere_5_0.1     0.789452    0.623651  0.051760465\n",
       "14    hemisphere_50_0.1     0.865310    4.576982   0.20637679\n",
       "15      saddle_none_0.1     0.933620    0.784032  0.018535296\n",
       "16        saddle_15_0.3     0.677027    1.541096   0.34323764\n",
       "17         saddle_5_0.1     0.785498    1.382261  0.029197698\n",
       "18    hemisphere_10_0.1     0.976843    2.880428  0.048764646\n",
       "19    hemisphere_15_0.3     0.003218    2.475684    0.7199272\n",
       "20    hemisphere_15_0.7     0.162641    3.684200    2.1859736\n",
       "21      hemisphere_15_0     0.979294    3.594204  0.013288553\n",
       "22     ellipsoid_15_0.5     0.556689   12.079306   0.56921166\n",
       "23     ellipsoid_10_0.1     0.468024    6.542765   0.14057913\n",
       "24      ellipsoid_5_0.1     0.501222    9.889479  0.059939913\n",
       "25     ellipsoid_15_0.7    -0.073554   12.685026    1.1486657\n",
       "26   ellipsoid_none_0.1     0.273180    5.330163    0.1547515\n",
       "27    hemisphere_15_0.1     0.564702    4.051459   0.08095882\n",
       "28  hemisphere_none_0.1     0.918701    0.422754   0.04186386\n",
       "29     ellipsoid_50_0.1     0.629106   25.590904   0.13367188\n",
       "30     ellipsoid_15_0.1     0.845484   19.020750   0.05549545\n",
       "31       ellipsoid_15_0     0.913999   13.506490  0.024221402\n",
       "32     ellipsoid_15_0.3     0.610557   14.293184   0.30313063\n",
       "33        saddle_10_0.1     0.882665    2.007844   0.08494645\n",
       "34        saddle_50_0.1     0.939317    5.284430   0.13991937"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ours.to_csv('geodesics_no_density.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hemisphere_15_0.5']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "failed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
