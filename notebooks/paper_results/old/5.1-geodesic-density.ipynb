{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxingzhis\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import scprep\n",
    "import pandas as pd\n",
    "sys.path.append('../../src/')\n",
    "from train import load_data\n",
    "# from diffusion import DiffusionModel\n",
    "# from evaluate import get_results\n",
    "from omegaconf import OmegaConf\n",
    "# from main import load_data, make_model\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import PillowWriter\n",
    "import torch\n",
    "from model2 import Autoencoder, Preprocessor, Discriminator\n",
    "import magic\n",
    "import torch\n",
    "import pathlib\n",
    "import copy\n",
    "\n",
    "import wandb\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import scprep\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from geodesic import jacobian, velocity, CondCurve, GeodesicBridgeOverfit\n",
    "from plotly3d.plot import scatter, trajectories\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from procrustes import Procrustes\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "wandb.login()\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distance_to_geodesic_criterion(predicted_geodesic, true_geodesic):\n",
    "    # the inputs here are single samples from a geodesic; should be shape num_samples x num_dims\n",
    "    # for each input point, we want the closest distance to any point on the true geodesic using the euclidean distance, torch.cdist\n",
    "    D = torch.cdist(predicted_geodesic, true_geodesic)\n",
    "    min_dists_to_true_geodesic = D.min(dim=1)[0]\n",
    "    # we take the mean of the squared distances\n",
    "    return torch.mean(min_dists_to_true_geodesic**2)\n",
    "# def distance_to_geodesic_criterion(\n",
    "#     predicted_geodesic:torch.Tensor, # size num_geodesics x num_samples x num_dims\n",
    "#     true_geodesic:torch.Tensor, # size num_geodesics num_samples x num_dims. But it's okay if the num_samples are different\n",
    "#     ):\n",
    "#     \"\"\"\n",
    "#     Mean of the squared distances from each predicted point to the closest point on the true geodesic\n",
    "#     \"\"\"\n",
    "#     dists = []\n",
    "#     for i in range(predicted_geodesic.shape[0]):\n",
    "#         dists.append(_distance_to_geodesic_criterion(predicted_geodesic[i], true_geodesic[i]))\n",
    "#     dists = torch.stack(dists)\n",
    "#     return dists.mean()\n",
    "\n",
    "def distance_to_geodesic_criterion_len(\n",
    "    predicted_geodesic:torch.Tensor, # size num_geodesics x num_samples x num_dims\n",
    "    true_geodesic:torch.Tensor, # size num_geodesics num_samples x num_dims. But it's okay if the num_samples are different\n",
    "    lengths=1.,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Mean of the squared distances from each predicted point to the closest point on the true geodesic\n",
    "    \"\"\"\n",
    "    dists = []\n",
    "    # for i in range(predicted_geodesic.shape[0]):\n",
    "    for i in range(len(predicted_geodesic)):\n",
    "        dists.append(_distance_to_geodesic_criterion(predicted_geodesic[i], true_geodesic[i]))\n",
    "    dists = torch.stack(dists)\n",
    "    dists = dists / lengths\n",
    "    return dists.mean()\n",
    "\n",
    "def distances_to_geodesic(\n",
    "    predicted_geodesic:torch.Tensor, # size num_geodesics x num_samples x num_dims\n",
    "    true_geodesic:torch.Tensor, # size num_geodesics num_samples x num_dims. But it's okay if the num_samples are different\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Mean of the squared distances from each predicted point to the closest point on the true geodesic\n",
    "    \"\"\"\n",
    "    dists = []\n",
    "    # for i in range(predicted_geodesic.shape[0]):\n",
    "    for i in range(len(predicted_geodesic)):\n",
    "        dists.append(_distance_to_geodesic_criterion(predicted_geodesic[i], true_geodesic[i]))\n",
    "    dists = torch.stack(dists)\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"xingzhis\"\n",
    "project = \"dmae\"\n",
    "# sweep_id = 'a72qr26q'\n",
    "sweep_id = 'dcxgbhjp'\n",
    "sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "runs_data = []\n",
    "\n",
    "# Iterate through each run in the sweep\n",
    "for run in sweep.runs:\n",
    "    # Extract metrics and configs\n",
    "    metrics = run.summary._json_dict\n",
    "    configs = run.config\n",
    "    \n",
    "    # Combine metrics and configs, and add run ID\n",
    "    combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "    \n",
    "    # Append the combined data to the list\n",
    "    runs_data.append(combined_data)\n",
    "\n",
    "# Create a DataFrame from the runs data\n",
    "df = pd.DataFrame(runs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df[(df['loss_epoch']!='NaN')][['data_name']].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['loss_epoch']=='NaN')][['data_name']].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]/gpfs/gibbs/pi/krishnaswamy_smita/xingzhi/.conda_envs/geosink/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'preprocessor' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['preprocessor'])`.\n",
      "  rank_zero_warn(\n",
      " 11%|█         | 4/36 [00:03<00:26,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) torus_15_0.1\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 14/36 [00:16<00:16,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found 8     tlxawe92\n",
      "10    tlxawe92\n",
      "Name: run_id, dtype: object saddle_10_0.1\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 18/36 [00:19<00:12,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) saddle_15_0.3\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 19/36 [00:20<00:10,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found 9     dp2zljfz\n",
      "11    dp2zljfz\n",
      "Name: run_id, dtype: object saddle_50_0.1\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 22/36 [00:22<00:07,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) hemisphere_15_0.3\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:34<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res_list = []\n",
    "missing = []\n",
    "failed = []\n",
    "\n",
    "for data_name in tqdm(names):\n",
    "    try:\n",
    "        entity = \"xingzhis\"\n",
    "        project = \"dmae\"\n",
    "        sweep_id = 'ys48kno0'\n",
    "        sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "\n",
    "        runs_data = []\n",
    "\n",
    "        # Iterate through each run in the sweep\n",
    "        for run in sweep.runs:\n",
    "            # Extract metrics and configs\n",
    "            metrics = run.summary._json_dict\n",
    "            configs = run.config\n",
    "            \n",
    "            # Combine metrics and configs, and add run ID\n",
    "            combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "            # Append the combined data to the list\n",
    "            runs_data.append(combined_data)\n",
    "\n",
    "        # Create a DataFrame from the runs data\n",
    "        df = pd.DataFrame(runs_data)\n",
    "\n",
    "        # run_ids = df[(df['data.name'] == data_name) & (df['cfg/loss/weights/cycle'] == 1.) & (df['cfg/dimensions/latent'] == 3)]['run_id']\n",
    "        run_ids = df[(df['data.name'] == data_name) & (df['loss.weights.cycle'] == 1.0) & (df['dimensions.latent'] == 3)]['run_id']\n",
    "        # assert len(run_ids) == 1\n",
    "        if len(run_ids) != 1:\n",
    "            print('AE not found:', run_ids, data_name)\n",
    "        run_id = run_ids.iloc[0]\n",
    "        run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        # run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        folder_path = '../../src/wandb/'\n",
    "        cfg = OmegaConf.create(run.config)\n",
    "        folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        ckpt_path = ckpt_files[0]\n",
    "        cfg.data.root = '../' + cfg.data.root\n",
    "        model = Autoencoder.load_from_checkpoint(ckpt_path)\n",
    "        data = np.load(f\"{cfg.data.root}/{cfg.data.name}{cfg.data.filetype}\", allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "        # sweep_id = 'ywep3ixr'\n",
    "        # sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "        # # Initialize an empty list to store run data\n",
    "        # runs_data = []\n",
    "\n",
    "        # # Iterate through each run in the sweep\n",
    "        # for run in sweep.runs:\n",
    "        #     # Extract metrics and configs\n",
    "        #     metrics = run.summary._json_dict\n",
    "        #     configs = run.config\n",
    "            \n",
    "        #     # Combine metrics and configs, and add run ID\n",
    "        #     combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "        #     # Append the combined data to the list\n",
    "        #     runs_data.append(combined_data)\n",
    "\n",
    "        # # Create a DataFrame from the runs data\n",
    "        # df = pd.DataFrame(runs_data)\n",
    "        # run_ids = df[(df['data.name'] == data_name)][['run_id']]\n",
    "        # assert len(run_ids) == 1\n",
    "        # run_id = run_ids.iloc[0]\n",
    "        # run = api.run(f\"{entity}/{project}/{run_ids.iloc[0].values[0]}\")\n",
    "        # folder_path = '../../src/wandb/'\n",
    "        # cfg = OmegaConf.create(run.config)\n",
    "        # folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        # ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        # ckpt_path = ckpt_files[0]\n",
    "        # cfg.data.root = '../' + cfg.data.root\n",
    "        # discriminator = Discriminator.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "\n",
    "\n",
    "        entity = \"xingzhis\"\n",
    "        project = \"dmae\"\n",
    "        sweep_id = 'a72qr26q'\n",
    "        sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "        runs_data = []\n",
    "\n",
    "        # Iterate through each run in the sweep\n",
    "        for run in sweep.runs:\n",
    "            # Extract metrics and configs\n",
    "            metrics = run.summary._json_dict\n",
    "            configs = run.config\n",
    "            \n",
    "            # Combine metrics and configs, and add run ID\n",
    "            combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "            # Append the combined data to the list\n",
    "            runs_data.append(combined_data)\n",
    "\n",
    "        # Create a DataFrame from the runs data\n",
    "        df = pd.DataFrame(runs_data)\n",
    "\n",
    "        run_ids = df[(df['data_name'] == data_name) & (df['loss_epoch'] != 'NaN') & (df['dimensions_latent'] == 3)]['run_id']\n",
    "        # assert len(run_ids) == 1\n",
    "        if len(run_ids) != 1:\n",
    "            print('Geod not found', run_ids, data_name)\n",
    "        run_id = run_ids.iloc[0]\n",
    "        run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        cfg_main = OmegaConf.create(run.config)\n",
    "        folder_path = '../geodesic_on_datasets//wandb/'\n",
    "        folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        ckpt_path = ckpt_files[0]\n",
    "\n",
    "        x = torch.tensor(data['data'], dtype=torch.float32, device=model.device)\n",
    "        xbatch = torch.tensor(data['start_points'], dtype=x.dtype, device=x.device)\n",
    "        xendbatch = torch.tensor(data['end_points'], dtype=x.dtype, device=x.device)\n",
    "        # xbatch = model.encoder.preprocessor.normalize(xbatch)\n",
    "        # xendbatch = model.encoder.preprocessor.normalize(xendbatch)\n",
    "        # if cfg_main.overfit:\n",
    "        #     ids = torch.eye(xbatch.size(0))\n",
    "        # else:\n",
    "        ids = torch.zeros((xbatch.size(0),1))\n",
    "        dataset = TensorDataset(xbatch, xendbatch, ids)\n",
    "        dataloader = DataLoader(dataset, batch_size=len(x), shuffle=False)\n",
    "\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # def func(x):\n",
    "        #     return model.encoder(x)\n",
    "        # for param in discriminator.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # def discriminator_func_for_grad(x):\n",
    "        #     return discriminator.positive_proba(x, normalize=False).reshape(-1,1)\n",
    "        # def discriminator_func(x):\n",
    "        #     return discriminator.positive_proba(x, normalize=False).reshape(-1,1)\n",
    "        ofm = lambda x: x\n",
    "        gbmodel = GeodesicBridgeOverfit.load_from_checkpoint(\n",
    "            checkpoint_path=ckpt_path,\n",
    "            func=ofm,\n",
    "            # func = enc_func,\n",
    "            # discriminator_func=disc_func_pen,\n",
    "            # discriminator_func_for_grad=discriminator_func_for_grad,\n",
    "            input_dim=x.size(1), \n",
    "            hidden_dim=64, \n",
    "            scale_factor=1, \n",
    "            symmetric=True, \n",
    "            num_layers=3, \n",
    "            n_tsteps=100, \n",
    "            lr=1e-3, \n",
    "            weight_decay=1e-3,\n",
    "            discriminator_weight=0.,\n",
    "            discriminator_func_for_grad_weight=0.,\n",
    "            id_dim=1,\n",
    "            id_emb_dim=1,\n",
    "            density_weight=0.,\n",
    "            length_weight=1.,\n",
    "        )\n",
    "\n",
    "        # batch = next(iter(dataloader))\n",
    "        # x0, x1, ids = batch\n",
    "        try:\n",
    "            data_gt = np.load(f\"/gpfs/gibbs/pi/krishnaswamy_smita/xingzhi/dmae/data/neurips_results/toy/gt/{data_name}.npz\", allow_pickle=True)\n",
    "        except:\n",
    "            print(f\"CANNOT FIND FILE {data_name}.npz\")\n",
    "            missing.append(data_name)\n",
    "            continue\n",
    "        xbatch = torch.tensor(data_gt['start_points'], dtype=x.dtype, device=x.device)\n",
    "        xendbatch = torch.tensor(data_gt['end_points'], dtype=x.dtype, device=x.device)\n",
    "        x0 = xbatch\n",
    "        x1 = xendbatch\n",
    "        # xbatch = model.encoder.preprocessor.normalize(xbatch)\n",
    "        # xendbatch = model.encoder.preprocessor.normalize(xendbatch)\n",
    "        ids = torch.zeros((xbatch.size(0),1))\n",
    "        # ids = torch.eye((xbatch.size(0)))\n",
    "\n",
    "        # dataset = TensorDataset(xbatch, xendbatch, ids)\n",
    "        # dataloader = DataLoader(dataset, batch_size=len(z), shuffle=True)\n",
    "\n",
    "        def cc_func(x0, x1, t):\n",
    "            return gbmodel.cc(x0, x1, t, ids)\n",
    "        vectors = velocity(cc_func, gbmodel.ts, x0, x1)\n",
    "        cc_pts = gbmodel.cc(x0, x1, gbmodel.ts, ids)\n",
    "        vectors_flat = vectors.flatten(0,1)\n",
    "        cc_pts_flat = cc_pts.flatten(0, 1)\n",
    "        jac_flat = jacobian(gbmodel.func, cc_pts_flat)\n",
    "        length_all = torch.sqrt((torch.einsum(\"nij,nj->ni\", jac_flat, vectors_flat)**2).sum(axis=1))\n",
    "        length_all = length_all.reshape(vectors.shape[0], vectors.shape[1])\n",
    "        length = length_all.mean(axis=0)\n",
    "\n",
    "        geods = (cc_pts_flat).reshape(cc_pts.shape)\n",
    "        length2 = torch.sqrt(torch.diff(geods, axis=0)**2).sum(axis=-1).sum(axis=0)\n",
    "\n",
    "\n",
    "        # plt.scatter(length2.detach().numpy(), data_gt['geodesic_lengths'])\n",
    "        # plt.title(data_name)\n",
    "        # plt.show()\n",
    "\n",
    "        # gt_len = torch.tensor(data_gt['geodesic_lengths'])\n",
    "        true_geod = torch.tensor(data_gt['geodesics'], dtype=torch.float32)\n",
    "        gt_len = torch.sqrt(torch.diff(true_geod.permute(1,0,2), axis=0)**2).sum(axis=-1).sum(axis=0)\n",
    "        corr = np.corrcoef(length.detach().numpy(), gt_len.cpu().numpy())[0,1]\n",
    "        mse = ((length.detach().numpy() - gt_len.cpu().numpy())**2).mean()\n",
    "        dist2geod = distance_to_geodesic_criterion_len(geods.permute(1,0,2), true_geod, lengths=gt_len).detach().numpy()\n",
    "\n",
    "        res_list.append(dict(\n",
    "            name=data_name,\n",
    "            length_corr=corr,\n",
    "            length_mse=mse,\n",
    "            dist2geod=dist2geod\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        failed.append(data_name)\n",
    "        print(e)\n",
    "\n",
    "res_df = pd.DataFrame(res_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['torus_15_0.1',\n",
       " 'saddle_10_0.1',\n",
       " 'saddle_15_0.3',\n",
       " 'saddle_50_0.1',\n",
       " 'hemisphere_15_0.3']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/gibbs/pi/krishnaswamy_smita/xingzhi/.conda_envs/geosink/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'preprocessor' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['preprocessor'])`.\n",
      "  rank_zero_warn(\n",
      " 20%|██        | 1/5 [00:00<00:00,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) torus_15_0.1\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:00<00:01,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) saddle_15_0.3\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) hemisphere_15_0.3\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res_list = []\n",
    "missing = []\n",
    "failed2 = []\n",
    "\n",
    "for data_name in tqdm(failed):\n",
    "    try:\n",
    "        entity = \"xingzhis\"\n",
    "        project = \"dmae\"\n",
    "        sweep_id = 'ys48kno0'\n",
    "        sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "\n",
    "        runs_data = []\n",
    "\n",
    "        # Iterate through each run in the sweep\n",
    "        for run in sweep.runs:\n",
    "            # Extract metrics and configs\n",
    "            metrics = run.summary._json_dict\n",
    "            configs = run.config\n",
    "            \n",
    "            # Combine metrics and configs, and add run ID\n",
    "            combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "            # Append the combined data to the list\n",
    "            runs_data.append(combined_data)\n",
    "\n",
    "        # Create a DataFrame from the runs data\n",
    "        df = pd.DataFrame(runs_data)\n",
    "\n",
    "        # run_ids = df[(df['data.name'] == data_name) & (df['cfg/loss/weights/cycle'] == 1.) & (df['cfg/dimensions/latent'] == 3)]['run_id']\n",
    "        run_ids = df[(df['data.name'] == data_name) & (df['loss.weights.cycle'] == 1.0) & (df['dimensions.latent'] == 3)]['run_id']\n",
    "        # assert len(run_ids) == 1\n",
    "        if len(run_ids) != 1:\n",
    "            print('AE not found:', run_ids, data_name)\n",
    "        run_id = run_ids.iloc[0]\n",
    "        run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        # run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        folder_path = '../../src/wandb/'\n",
    "        cfg = OmegaConf.create(run.config)\n",
    "        folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        ckpt_path = ckpt_files[0]\n",
    "        cfg.data.root = '../' + cfg.data.root\n",
    "        model = Autoencoder.load_from_checkpoint(ckpt_path)\n",
    "        data = np.load(f\"{cfg.data.root}/{cfg.data.name}{cfg.data.filetype}\", allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "        # sweep_id = 'ywep3ixr'\n",
    "        # sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "        # # Initialize an empty list to store run data\n",
    "        # runs_data = []\n",
    "\n",
    "        # # Iterate through each run in the sweep\n",
    "        # for run in sweep.runs:\n",
    "        #     # Extract metrics and configs\n",
    "        #     metrics = run.summary._json_dict\n",
    "        #     configs = run.config\n",
    "            \n",
    "        #     # Combine metrics and configs, and add run ID\n",
    "        #     combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "        #     # Append the combined data to the list\n",
    "        #     runs_data.append(combined_data)\n",
    "\n",
    "        # # Create a DataFrame from the runs data\n",
    "        # df = pd.DataFrame(runs_data)\n",
    "        # run_ids = df[(df['data.name'] == data_name)][['run_id']]\n",
    "        # assert len(run_ids) == 1\n",
    "        # run_id = run_ids.iloc[0]\n",
    "        # run = api.run(f\"{entity}/{project}/{run_ids.iloc[0].values[0]}\")\n",
    "        # folder_path = '../../src/wandb/'\n",
    "        # cfg = OmegaConf.create(run.config)\n",
    "        # folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        # ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        # ckpt_path = ckpt_files[0]\n",
    "        # cfg.data.root = '../' + cfg.data.root\n",
    "        # discriminator = Discriminator.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "\n",
    "\n",
    "        entity = \"xingzhis\"\n",
    "        project = \"dmae\"\n",
    "        sweep_id = '88x8glfh'\n",
    "        sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "        runs_data = []\n",
    "\n",
    "        # Iterate through each run in the sweep\n",
    "        for run in sweep.runs:\n",
    "            # Extract metrics and configs\n",
    "            metrics = run.summary._json_dict\n",
    "            configs = run.config\n",
    "            \n",
    "            # Combine metrics and configs, and add run ID\n",
    "            combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "            # Append the combined data to the list\n",
    "            runs_data.append(combined_data)\n",
    "\n",
    "        # Create a DataFrame from the runs data\n",
    "        df = pd.DataFrame(runs_data)\n",
    "\n",
    "        run_ids = df[(df['data_name'] == data_name) & (df['loss_epoch'] != 'NaN') & (df['dimensions_latent'] == 3)]['run_id']\n",
    "        # assert len(run_ids) == 1\n",
    "        if len(run_ids) != 1:\n",
    "            print('Geod not found', run_ids, data_name)\n",
    "        run_id = run_ids.iloc[0]\n",
    "        run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        cfg_main = OmegaConf.create(run.config)\n",
    "        folder_path = '../geodesic_on_datasets//wandb/'\n",
    "        folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        ckpt_path = ckpt_files[0]\n",
    "\n",
    "        x = torch.tensor(data['data'], dtype=torch.float32, device=model.device)\n",
    "        xbatch = torch.tensor(data['start_points'], dtype=x.dtype, device=x.device)\n",
    "        xendbatch = torch.tensor(data['end_points'], dtype=x.dtype, device=x.device)\n",
    "        # xbatch = model.encoder.preprocessor.normalize(xbatch)\n",
    "        # xendbatch = model.encoder.preprocessor.normalize(xendbatch)\n",
    "        # if cfg_main.overfit:\n",
    "        #     ids = torch.eye(xbatch.size(0))\n",
    "        # else:\n",
    "        ids = torch.zeros((xbatch.size(0),1))\n",
    "        dataset = TensorDataset(xbatch, xendbatch, ids)\n",
    "        dataloader = DataLoader(dataset, batch_size=len(x), shuffle=False)\n",
    "\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # def func(x):\n",
    "        #     return model.encoder(x)\n",
    "        # for param in discriminator.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # def discriminator_func_for_grad(x):\n",
    "        #     return discriminator.positive_proba(x, normalize=False).reshape(-1,1)\n",
    "        # def discriminator_func(x):\n",
    "        #     return discriminator.positive_proba(x, normalize=False).reshape(-1,1)\n",
    "        ofm = lambda x: x\n",
    "        gbmodel = GeodesicBridgeOverfit.load_from_checkpoint(\n",
    "            checkpoint_path=ckpt_path,\n",
    "            func=ofm,\n",
    "            # func = enc_func,\n",
    "            # discriminator_func=disc_func_pen,\n",
    "            # discriminator_func_for_grad=discriminator_func_for_grad,\n",
    "            input_dim=x.size(1), \n",
    "            hidden_dim=64, \n",
    "            scale_factor=1, \n",
    "            symmetric=True, \n",
    "            num_layers=3, \n",
    "            n_tsteps=100, \n",
    "            lr=1e-3, \n",
    "            weight_decay=1e-3,\n",
    "            discriminator_weight=0.,\n",
    "            discriminator_func_for_grad_weight=0.,\n",
    "            id_dim=1,\n",
    "            id_emb_dim=1,\n",
    "            density_weight=0.,\n",
    "            length_weight=1.,\n",
    "        )\n",
    "\n",
    "        # batch = next(iter(dataloader))\n",
    "        # x0, x1, ids = batch\n",
    "        try:\n",
    "            data_gt = np.load(f\"/gpfs/gibbs/pi/krishnaswamy_smita/xingzhi/dmae/data/neurips_results/toy/gt/{data_name}.npz\", allow_pickle=True)\n",
    "        except:\n",
    "            print(f\"CANNOT FIND FILE {data_name}.npz\")\n",
    "            missing.append(data_name)\n",
    "            continue\n",
    "        xbatch = torch.tensor(data_gt['start_points'], dtype=x.dtype, device=x.device)\n",
    "        xendbatch = torch.tensor(data_gt['end_points'], dtype=x.dtype, device=x.device)\n",
    "        x0 = xbatch\n",
    "        x1 = xendbatch\n",
    "        # xbatch = model.encoder.preprocessor.normalize(xbatch)\n",
    "        # xendbatch = model.encoder.preprocessor.normalize(xendbatch)\n",
    "        ids = torch.zeros((xbatch.size(0),1))\n",
    "        # ids = torch.eye((xbatch.size(0)))\n",
    "\n",
    "        # dataset = TensorDataset(xbatch, xendbatch, ids)\n",
    "        # dataloader = DataLoader(dataset, batch_size=len(z), shuffle=True)\n",
    "\n",
    "        def cc_func(x0, x1, t):\n",
    "            return gbmodel.cc(x0, x1, t, ids)\n",
    "        vectors = velocity(cc_func, gbmodel.ts, x0, x1)\n",
    "        cc_pts = gbmodel.cc(x0, x1, gbmodel.ts, ids)\n",
    "        vectors_flat = vectors.flatten(0,1)\n",
    "        cc_pts_flat = cc_pts.flatten(0, 1)\n",
    "        jac_flat = jacobian(gbmodel.func, cc_pts_flat)\n",
    "        length_all = torch.sqrt((torch.einsum(\"nij,nj->ni\", jac_flat, vectors_flat)**2).sum(axis=1))\n",
    "        length_all = length_all.reshape(vectors.shape[0], vectors.shape[1])\n",
    "        length = length_all.mean(axis=0)\n",
    "\n",
    "        geods = (cc_pts_flat).reshape(cc_pts.shape)\n",
    "        length2 = torch.sqrt(torch.diff(geods, axis=0)**2).sum(axis=-1).sum(axis=0)\n",
    "\n",
    "\n",
    "        # plt.scatter(length2.detach().numpy(), data_gt['geodesic_lengths'])\n",
    "        # plt.title(data_name)\n",
    "        # plt.show()\n",
    "\n",
    "        # gt_len = torch.tensor(data_gt['geodesic_lengths'])\n",
    "        true_geod = torch.tensor(data_gt['geodesics'], dtype=torch.float32)\n",
    "        gt_len = torch.sqrt(torch.diff(true_geod.permute(1,0,2), axis=0)**2).sum(axis=-1).sum(axis=0)\n",
    "        corr = np.corrcoef(length.detach().numpy(), gt_len.cpu().numpy())[0,1]\n",
    "        mse = ((length.detach().numpy() - gt_len.cpu().numpy())**2).mean()\n",
    "        dist2geod = distance_to_geodesic_criterion_len(geods.permute(1,0,2), true_geod, lengths=gt_len).detach().numpy()\n",
    "\n",
    "        res_list.append(dict(\n",
    "            name=data_name,\n",
    "            length_corr=corr,\n",
    "            length_mse=mse,\n",
    "            dist2geod=dist2geod\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        failed2.append(data_name)\n",
    "        print(e)\n",
    "\n",
    "res_df2 = pd.DataFrame(res_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ours = pd.concat([res_df, res_df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>length_corr</th>\n",
       "      <th>length_mse</th>\n",
       "      <th>dist2geod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ellipsoid_10_0.1</td>\n",
       "      <td>0.336414</td>\n",
       "      <td>7.626499</td>\n",
       "      <td>0.29572147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ellipsoid_15_0</td>\n",
       "      <td>0.921134</td>\n",
       "      <td>13.538965</td>\n",
       "      <td>0.022677947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ellipsoid_15_0.1</td>\n",
       "      <td>0.820483</td>\n",
       "      <td>18.791481</td>\n",
       "      <td>0.06582518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ellipsoid_15_0.3</td>\n",
       "      <td>0.576552</td>\n",
       "      <td>13.403852</td>\n",
       "      <td>0.32661584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ellipsoid_15_0.5</td>\n",
       "      <td>0.647232</td>\n",
       "      <td>10.757952</td>\n",
       "      <td>0.6226368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ellipsoid_15_0.7</td>\n",
       "      <td>0.060474</td>\n",
       "      <td>11.378612</td>\n",
       "      <td>1.1153214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ellipsoid_50_0.1</td>\n",
       "      <td>0.566694</td>\n",
       "      <td>24.697193</td>\n",
       "      <td>0.15313408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ellipsoid_5_0.1</td>\n",
       "      <td>0.544357</td>\n",
       "      <td>10.093882</td>\n",
       "      <td>0.06106465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ellipsoid_none_0.1</td>\n",
       "      <td>0.377259</td>\n",
       "      <td>4.839061</td>\n",
       "      <td>0.13107541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hemisphere_10_0.1</td>\n",
       "      <td>0.833427</td>\n",
       "      <td>2.442398</td>\n",
       "      <td>0.059005648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hemisphere_15_0</td>\n",
       "      <td>0.981382</td>\n",
       "      <td>3.575214</td>\n",
       "      <td>0.011176046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hemisphere_15_0.1</td>\n",
       "      <td>0.674747</td>\n",
       "      <td>3.646724</td>\n",
       "      <td>0.092019066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hemisphere_15_0.5</td>\n",
       "      <td>0.030274</td>\n",
       "      <td>2.069340</td>\n",
       "      <td>1.115855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hemisphere_15_0.7</td>\n",
       "      <td>0.156693</td>\n",
       "      <td>4.128926</td>\n",
       "      <td>2.273868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hemisphere_50_0.1</td>\n",
       "      <td>0.915925</td>\n",
       "      <td>4.698455</td>\n",
       "      <td>0.20870349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hemisphere_5_0.1</td>\n",
       "      <td>0.882661</td>\n",
       "      <td>0.521056</td>\n",
       "      <td>0.048017785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>hemisphere_none_0.1</td>\n",
       "      <td>0.768056</td>\n",
       "      <td>0.532315</td>\n",
       "      <td>0.092449844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>saddle_10_0.1</td>\n",
       "      <td>0.936848</td>\n",
       "      <td>2.027088</td>\n",
       "      <td>0.07527998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>saddle_15_0</td>\n",
       "      <td>0.973656</td>\n",
       "      <td>5.030990</td>\n",
       "      <td>0.0126630645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>saddle_15_0.1</td>\n",
       "      <td>0.824788</td>\n",
       "      <td>3.760745</td>\n",
       "      <td>0.05282901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>saddle_15_0.5</td>\n",
       "      <td>0.437962</td>\n",
       "      <td>1.005835</td>\n",
       "      <td>0.8284405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>saddle_15_0.7</td>\n",
       "      <td>0.344530</td>\n",
       "      <td>2.878682</td>\n",
       "      <td>1.804467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>saddle_50_0.1</td>\n",
       "      <td>0.938123</td>\n",
       "      <td>5.246984</td>\n",
       "      <td>0.14835839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>saddle_5_0.1</td>\n",
       "      <td>0.928094</td>\n",
       "      <td>1.374258</td>\n",
       "      <td>0.023082498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>saddle_none_0.1</td>\n",
       "      <td>0.918670</td>\n",
       "      <td>0.755212</td>\n",
       "      <td>0.022982907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>torus_10_0.1</td>\n",
       "      <td>0.906720</td>\n",
       "      <td>15.638974</td>\n",
       "      <td>0.06293617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>torus_15_0</td>\n",
       "      <td>0.978895</td>\n",
       "      <td>25.533764</td>\n",
       "      <td>0.025190946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>torus_15_0.3</td>\n",
       "      <td>0.940678</td>\n",
       "      <td>23.389124</td>\n",
       "      <td>0.22142553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>torus_15_0.5</td>\n",
       "      <td>0.851181</td>\n",
       "      <td>18.345373</td>\n",
       "      <td>0.5162952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>torus_15_0.7</td>\n",
       "      <td>0.558442</td>\n",
       "      <td>14.995730</td>\n",
       "      <td>0.9642704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>torus_50_0.1</td>\n",
       "      <td>0.917187</td>\n",
       "      <td>34.002651</td>\n",
       "      <td>0.1104999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>torus_5_0.1</td>\n",
       "      <td>0.890268</td>\n",
       "      <td>9.415816</td>\n",
       "      <td>0.05384674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>torus_none_0.1</td>\n",
       "      <td>0.924677</td>\n",
       "      <td>4.056273</td>\n",
       "      <td>0.11373744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  length_corr  length_mse     dist2geod\n",
       "21     ellipsoid_10_0.1     0.336414    7.626499    0.29572147\n",
       "29       ellipsoid_15_0     0.921134   13.538965   0.022677947\n",
       "28     ellipsoid_15_0.1     0.820483   18.791481    0.06582518\n",
       "30     ellipsoid_15_0.3     0.576552   13.403852    0.32661584\n",
       "19     ellipsoid_15_0.5     0.647232   10.757952     0.6226368\n",
       "23     ellipsoid_15_0.7     0.060474   11.378612     1.1153214\n",
       "27     ellipsoid_50_0.1     0.566694   24.697193    0.15313408\n",
       "22      ellipsoid_5_0.1     0.544357   10.093882    0.06106465\n",
       "24   ellipsoid_none_0.1     0.377259    4.839061    0.13107541\n",
       "16    hemisphere_10_0.1     0.833427    2.442398   0.059005648\n",
       "18      hemisphere_15_0     0.981382    3.575214   0.011176046\n",
       "25    hemisphere_15_0.1     0.674747    3.646724   0.092019066\n",
       "20    hemisphere_15_0.5     0.030274    2.069340      1.115855\n",
       "17    hemisphere_15_0.7     0.156693    4.128926      2.273868\n",
       "13    hemisphere_50_0.1     0.915925    4.698455    0.20870349\n",
       "12     hemisphere_5_0.1     0.882661    0.521056   0.048017785\n",
       "26  hemisphere_none_0.1     0.768056    0.532315   0.092449844\n",
       "31        saddle_10_0.1     0.936848    2.027088    0.07527998\n",
       "9           saddle_15_0     0.973656    5.030990  0.0126630645\n",
       "7         saddle_15_0.1     0.824788    3.760745    0.05282901\n",
       "11        saddle_15_0.5     0.437962    1.005835     0.8284405\n",
       "8         saddle_15_0.7     0.344530    2.878682      1.804467\n",
       "32        saddle_50_0.1     0.938123    5.246984    0.14835839\n",
       "15         saddle_5_0.1     0.928094    1.374258   0.023082498\n",
       "14      saddle_none_0.1     0.918670    0.755212   0.022982907\n",
       "6          torus_10_0.1     0.906720   15.638974    0.06293617\n",
       "10           torus_15_0     0.978895   25.533764   0.025190946\n",
       "4          torus_15_0.3     0.940678   23.389124    0.22142553\n",
       "5          torus_15_0.5     0.851181   18.345373     0.5162952\n",
       "2          torus_15_0.7     0.558442   14.995730     0.9642704\n",
       "3          torus_50_0.1     0.917187   34.002651     0.1104999\n",
       "0           torus_5_0.1     0.890268    9.415816    0.05384674\n",
       "1        torus_none_0.1     0.924677    4.056273    0.11373744"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ours.sort_values('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>length_corr</th>\n",
       "      <th>length_mse</th>\n",
       "      <th>dist2geod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>torus_5_0.1</td>\n",
       "      <td>0.890268</td>\n",
       "      <td>9.415816</td>\n",
       "      <td>0.05384674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>torus_none_0.1</td>\n",
       "      <td>0.924677</td>\n",
       "      <td>4.056273</td>\n",
       "      <td>0.11373744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>torus_15_0.7</td>\n",
       "      <td>0.558442</td>\n",
       "      <td>14.995730</td>\n",
       "      <td>0.9642704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>torus_50_0.1</td>\n",
       "      <td>0.917187</td>\n",
       "      <td>34.002651</td>\n",
       "      <td>0.1104999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>torus_15_0.3</td>\n",
       "      <td>0.940678</td>\n",
       "      <td>23.389124</td>\n",
       "      <td>0.22142553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>torus_15_0.5</td>\n",
       "      <td>0.851181</td>\n",
       "      <td>18.345373</td>\n",
       "      <td>0.5162952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>torus_10_0.1</td>\n",
       "      <td>0.906720</td>\n",
       "      <td>15.638974</td>\n",
       "      <td>0.06293617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>saddle_15_0.1</td>\n",
       "      <td>0.824788</td>\n",
       "      <td>3.760745</td>\n",
       "      <td>0.05282901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>saddle_15_0.7</td>\n",
       "      <td>0.344530</td>\n",
       "      <td>2.878682</td>\n",
       "      <td>1.804467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>saddle_15_0</td>\n",
       "      <td>0.973656</td>\n",
       "      <td>5.030990</td>\n",
       "      <td>0.0126630645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>torus_15_0</td>\n",
       "      <td>0.978895</td>\n",
       "      <td>25.533764</td>\n",
       "      <td>0.025190946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>saddle_15_0.5</td>\n",
       "      <td>0.437962</td>\n",
       "      <td>1.005835</td>\n",
       "      <td>0.8284405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hemisphere_5_0.1</td>\n",
       "      <td>0.882661</td>\n",
       "      <td>0.521056</td>\n",
       "      <td>0.048017785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hemisphere_50_0.1</td>\n",
       "      <td>0.915925</td>\n",
       "      <td>4.698455</td>\n",
       "      <td>0.20870349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>saddle_none_0.1</td>\n",
       "      <td>0.918670</td>\n",
       "      <td>0.755212</td>\n",
       "      <td>0.022982907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>saddle_5_0.1</td>\n",
       "      <td>0.928094</td>\n",
       "      <td>1.374258</td>\n",
       "      <td>0.023082498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hemisphere_10_0.1</td>\n",
       "      <td>0.833427</td>\n",
       "      <td>2.442398</td>\n",
       "      <td>0.059005648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hemisphere_15_0.7</td>\n",
       "      <td>0.156693</td>\n",
       "      <td>4.128926</td>\n",
       "      <td>2.273868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hemisphere_15_0</td>\n",
       "      <td>0.981382</td>\n",
       "      <td>3.575214</td>\n",
       "      <td>0.011176046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ellipsoid_15_0.5</td>\n",
       "      <td>0.647232</td>\n",
       "      <td>10.757952</td>\n",
       "      <td>0.6226368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hemisphere_15_0.5</td>\n",
       "      <td>0.030274</td>\n",
       "      <td>2.069340</td>\n",
       "      <td>1.115855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ellipsoid_10_0.1</td>\n",
       "      <td>0.336414</td>\n",
       "      <td>7.626499</td>\n",
       "      <td>0.29572147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ellipsoid_5_0.1</td>\n",
       "      <td>0.544357</td>\n",
       "      <td>10.093882</td>\n",
       "      <td>0.06106465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ellipsoid_15_0.7</td>\n",
       "      <td>0.060474</td>\n",
       "      <td>11.378612</td>\n",
       "      <td>1.1153214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ellipsoid_none_0.1</td>\n",
       "      <td>0.377259</td>\n",
       "      <td>4.839061</td>\n",
       "      <td>0.13107541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hemisphere_15_0.1</td>\n",
       "      <td>0.674747</td>\n",
       "      <td>3.646724</td>\n",
       "      <td>0.092019066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>hemisphere_none_0.1</td>\n",
       "      <td>0.768056</td>\n",
       "      <td>0.532315</td>\n",
       "      <td>0.092449844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ellipsoid_50_0.1</td>\n",
       "      <td>0.566694</td>\n",
       "      <td>24.697193</td>\n",
       "      <td>0.15313408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ellipsoid_15_0.1</td>\n",
       "      <td>0.820483</td>\n",
       "      <td>18.791481</td>\n",
       "      <td>0.06582518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ellipsoid_15_0</td>\n",
       "      <td>0.921134</td>\n",
       "      <td>13.538965</td>\n",
       "      <td>0.022677947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ellipsoid_15_0.3</td>\n",
       "      <td>0.576552</td>\n",
       "      <td>13.403852</td>\n",
       "      <td>0.32661584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>saddle_10_0.1</td>\n",
       "      <td>0.936848</td>\n",
       "      <td>2.027088</td>\n",
       "      <td>0.07527998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>saddle_50_0.1</td>\n",
       "      <td>0.938123</td>\n",
       "      <td>5.246984</td>\n",
       "      <td>0.14835839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  length_corr  length_mse     dist2geod\n",
       "0           torus_5_0.1     0.890268    9.415816    0.05384674\n",
       "1        torus_none_0.1     0.924677    4.056273    0.11373744\n",
       "2          torus_15_0.7     0.558442   14.995730     0.9642704\n",
       "3          torus_50_0.1     0.917187   34.002651     0.1104999\n",
       "4          torus_15_0.3     0.940678   23.389124    0.22142553\n",
       "5          torus_15_0.5     0.851181   18.345373     0.5162952\n",
       "6          torus_10_0.1     0.906720   15.638974    0.06293617\n",
       "7         saddle_15_0.1     0.824788    3.760745    0.05282901\n",
       "8         saddle_15_0.7     0.344530    2.878682      1.804467\n",
       "9           saddle_15_0     0.973656    5.030990  0.0126630645\n",
       "10           torus_15_0     0.978895   25.533764   0.025190946\n",
       "11        saddle_15_0.5     0.437962    1.005835     0.8284405\n",
       "12     hemisphere_5_0.1     0.882661    0.521056   0.048017785\n",
       "13    hemisphere_50_0.1     0.915925    4.698455    0.20870349\n",
       "14      saddle_none_0.1     0.918670    0.755212   0.022982907\n",
       "15         saddle_5_0.1     0.928094    1.374258   0.023082498\n",
       "16    hemisphere_10_0.1     0.833427    2.442398   0.059005648\n",
       "17    hemisphere_15_0.7     0.156693    4.128926      2.273868\n",
       "18      hemisphere_15_0     0.981382    3.575214   0.011176046\n",
       "19     ellipsoid_15_0.5     0.647232   10.757952     0.6226368\n",
       "20    hemisphere_15_0.5     0.030274    2.069340      1.115855\n",
       "21     ellipsoid_10_0.1     0.336414    7.626499    0.29572147\n",
       "22      ellipsoid_5_0.1     0.544357   10.093882    0.06106465\n",
       "23     ellipsoid_15_0.7     0.060474   11.378612     1.1153214\n",
       "24   ellipsoid_none_0.1     0.377259    4.839061    0.13107541\n",
       "25    hemisphere_15_0.1     0.674747    3.646724   0.092019066\n",
       "26  hemisphere_none_0.1     0.768056    0.532315   0.092449844\n",
       "27     ellipsoid_50_0.1     0.566694   24.697193    0.15313408\n",
       "28     ellipsoid_15_0.1     0.820483   18.791481    0.06582518\n",
       "29       ellipsoid_15_0     0.921134   13.538965   0.022677947\n",
       "30     ellipsoid_15_0.3     0.576552   13.403852    0.32661584\n",
       "31        saddle_10_0.1     0.936848    2.027088    0.07527998\n",
       "32        saddle_50_0.1     0.938123    5.246984    0.14835839"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ours.to_csv('geodesics_density.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['torus_15_0.1', 'saddle_15_0.3', 'hemisphere_15_0.3']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "failed2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
