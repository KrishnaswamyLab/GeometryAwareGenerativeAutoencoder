{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxingzhis\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import scprep\n",
    "import pandas as pd\n",
    "sys.path.append('../../src/')\n",
    "from train import load_data\n",
    "# from diffusion import DiffusionModel\n",
    "# from evaluate import get_results\n",
    "from omegaconf import OmegaConf\n",
    "# from main import load_data, make_model\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import PillowWriter\n",
    "import torch\n",
    "from model2 import Autoencoder, Preprocessor, Discriminator\n",
    "import magic\n",
    "import torch\n",
    "import pathlib\n",
    "import copy\n",
    "\n",
    "import wandb\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import scprep\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from geodesic import jacobian, velocity, CondCurve, GeodesicBridgeOverfit\n",
    "from plotly3d.plot import scatter, trajectories\n",
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from procrustes import Procrustes\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "wandb.login()\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distance_to_geodesic_criterion(predicted_geodesic, true_geodesic):\n",
    "    # the inputs here are single samples from a geodesic; should be shape num_samples x num_dims\n",
    "    # for each input point, we want the closest distance to any point on the true geodesic using the euclidean distance, torch.cdist\n",
    "    D = torch.cdist(predicted_geodesic, true_geodesic)\n",
    "    min_dists_to_true_geodesic = D.min(dim=1)[0]\n",
    "    # we take the mean of the squared distances\n",
    "    return torch.mean(min_dists_to_true_geodesic**2)\n",
    "# def distance_to_geodesic_criterion(\n",
    "#     predicted_geodesic:torch.Tensor, # size num_geodesics x num_samples x num_dims\n",
    "#     true_geodesic:torch.Tensor, # size num_geodesics num_samples x num_dims. But it's okay if the num_samples are different\n",
    "#     ):\n",
    "#     \"\"\"\n",
    "#     Mean of the squared distances from each predicted point to the closest point on the true geodesic\n",
    "#     \"\"\"\n",
    "#     dists = []\n",
    "#     for i in range(predicted_geodesic.shape[0]):\n",
    "#         dists.append(_distance_to_geodesic_criterion(predicted_geodesic[i], true_geodesic[i]))\n",
    "#     dists = torch.stack(dists)\n",
    "#     return dists.mean()\n",
    "\n",
    "def distance_to_geodesic_criterion_len(\n",
    "    predicted_geodesic:torch.Tensor, # size num_geodesics x num_samples x num_dims\n",
    "    true_geodesic:torch.Tensor, # size num_geodesics num_samples x num_dims. But it's okay if the num_samples are different\n",
    "    lengths=1.,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Mean of the squared distances from each predicted point to the closest point on the true geodesic\n",
    "    \"\"\"\n",
    "    dists = []\n",
    "    # for i in range(predicted_geodesic.shape[0]):\n",
    "    for i in range(len(predicted_geodesic)):\n",
    "        dists.append(_distance_to_geodesic_criterion(predicted_geodesic[i], true_geodesic[i]))\n",
    "    dists = torch.stack(dists)\n",
    "    dists = dists / lengths\n",
    "    return dists.mean()\n",
    "\n",
    "def distances_to_geodesic(\n",
    "    predicted_geodesic:torch.Tensor, # size num_geodesics x num_samples x num_dims\n",
    "    true_geodesic:torch.Tensor, # size num_geodesics num_samples x num_dims. But it's okay if the num_samples are different\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Mean of the squared distances from each predicted point to the closest point on the true geodesic\n",
    "    \"\"\"\n",
    "    dists = []\n",
    "    # for i in range(predicted_geodesic.shape[0]):\n",
    "    for i in range(len(predicted_geodesic)):\n",
    "        dists.append(_distance_to_geodesic_criterion(predicted_geodesic[i], true_geodesic[i]))\n",
    "    dists = torch.stack(dists)\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"xingzhis\"\n",
    "project = \"dmae\"\n",
    "# sweep_id = 'a72qr26q'\n",
    "sweep_id = 'dcxgbhjp'\n",
    "sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "runs_data = []\n",
    "\n",
    "# Iterate through each run in the sweep\n",
    "for run in sweep.runs:\n",
    "    # Extract metrics and configs\n",
    "    metrics = run.summary._json_dict\n",
    "    configs = run.config\n",
    "    \n",
    "    # Combine metrics and configs, and add run ID\n",
    "    combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "    \n",
    "    # Append the combined data to the list\n",
    "    runs_data.append(combined_data)\n",
    "\n",
    "# Create a DataFrame from the runs data\n",
    "df = pd.DataFrame(runs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df[(df['loss_epoch']!='NaN')][['data_name']].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['loss_epoch']=='NaN')][['data_name']].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]/gpfs/gibbs/pi/krishnaswamy_smita/xingzhi/.conda_envs/geosink/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'preprocessor' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['preprocessor'])`.\n",
      "  rank_zero_warn(\n",
      "  6%|▌         | 2/36 [00:00<00:06,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) torus_15_0.7\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) torus_15_0.1\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 5/36 [00:00<00:04,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) torus_50_0.1\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 6/36 [00:01<00:06,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) torus_15_0.5\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 8/36 [00:01<00:05,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) saddle_15_0.1\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 10/36 [00:02<00:06,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) saddle_15_0\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 13/36 [00:03<00:07,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 18/36 [00:06<00:09,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 21/36 [00:06<00:05,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) hemisphere_15_0.3\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 24/36 [00:07<00:04,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) ellipsoid_15_0.5\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:13<00:00,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res_list = []\n",
    "missing = []\n",
    "failed = []\n",
    "\n",
    "for data_name in tqdm(names):\n",
    "    try:\n",
    "        entity = \"xingzhis\"\n",
    "        project = \"dmae\"\n",
    "        sweep_id = 'ys48kno0'\n",
    "        sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "\n",
    "        runs_data = []\n",
    "\n",
    "        # Iterate through each run in the sweep\n",
    "        for run in sweep.runs:\n",
    "            # Extract metrics and configs\n",
    "            metrics = run.summary._json_dict\n",
    "            configs = run.config\n",
    "            \n",
    "            # Combine metrics and configs, and add run ID\n",
    "            combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "            # Append the combined data to the list\n",
    "            runs_data.append(combined_data)\n",
    "\n",
    "        # Create a DataFrame from the runs data\n",
    "        df = pd.DataFrame(runs_data)\n",
    "\n",
    "        # run_ids = df[(df['data.name'] == data_name) & (df['cfg/loss/weights/cycle'] == 1.) & (df['cfg/dimensions/latent'] == 3)]['run_id']\n",
    "        run_ids = df[(df['data.name'] == data_name) & (df['loss.weights.cycle'] == 1.0) & (df['dimensions.latent'] == 3)]['run_id']\n",
    "        # assert len(run_ids) == 1\n",
    "        if len(run_ids) != 1:\n",
    "            print('AE not found:', run_ids, data_name)\n",
    "        run_id = run_ids.iloc[0]\n",
    "        run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        # run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        folder_path = '../../src/wandb/'\n",
    "        cfg = OmegaConf.create(run.config)\n",
    "        folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        ckpt_path = ckpt_files[0]\n",
    "        cfg.data.root = '../' + cfg.data.root\n",
    "        model = Autoencoder.load_from_checkpoint(ckpt_path)\n",
    "        data = np.load(f\"{cfg.data.root}/{cfg.data.name}{cfg.data.filetype}\", allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "        # sweep_id = 'ywep3ixr'\n",
    "        # sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "        # # Initialize an empty list to store run data\n",
    "        # runs_data = []\n",
    "\n",
    "        # # Iterate through each run in the sweep\n",
    "        # for run in sweep.runs:\n",
    "        #     # Extract metrics and configs\n",
    "        #     metrics = run.summary._json_dict\n",
    "        #     configs = run.config\n",
    "            \n",
    "        #     # Combine metrics and configs, and add run ID\n",
    "        #     combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "        #     # Append the combined data to the list\n",
    "        #     runs_data.append(combined_data)\n",
    "\n",
    "        # # Create a DataFrame from the runs data\n",
    "        # df = pd.DataFrame(runs_data)\n",
    "        # run_ids = df[(df['data.name'] == data_name)][['run_id']]\n",
    "        # assert len(run_ids) == 1\n",
    "        # run_id = run_ids.iloc[0]\n",
    "        # run = api.run(f\"{entity}/{project}/{run_ids.iloc[0].values[0]}\")\n",
    "        # folder_path = '../../src/wandb/'\n",
    "        # cfg = OmegaConf.create(run.config)\n",
    "        # folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        # ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        # ckpt_path = ckpt_files[0]\n",
    "        # cfg.data.root = '../' + cfg.data.root\n",
    "        # discriminator = Discriminator.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "\n",
    "\n",
    "        entity = \"xingzhis\"\n",
    "        project = \"dmae\"\n",
    "        # sweep_id = 'a72qr26q'\n",
    "        sweep_id = 'o5lf5v7t'\n",
    "        sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "        runs_data = []\n",
    "\n",
    "        # Iterate through each run in the sweep\n",
    "        for run in sweep.runs:\n",
    "            # Extract metrics and configs\n",
    "            metrics = run.summary._json_dict\n",
    "            configs = run.config\n",
    "            \n",
    "            # Combine metrics and configs, and add run ID\n",
    "            combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "            # Append the combined data to the list\n",
    "            runs_data.append(combined_data)\n",
    "\n",
    "        # Create a DataFrame from the runs data\n",
    "        df = pd.DataFrame(runs_data)\n",
    "\n",
    "        run_ids = df[(df['data_name'] == data_name) & (df['loss_epoch'] != 'NaN') & (df['dimensions_latent'] == 3)]['run_id']\n",
    "        # assert len(run_ids) == 1\n",
    "        if len(run_ids) < 1:\n",
    "            print('Geod not found', run_ids, data_name)\n",
    "        run_id = run_ids.iloc[0]\n",
    "        run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        cfg_main = OmegaConf.create(run.config)\n",
    "        folder_path = '../geodesic_on_datasets//wandb/'\n",
    "        folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        ckpt_path = ckpt_files[0]\n",
    "\n",
    "        x = torch.tensor(data['data'], dtype=torch.float32, device=model.device)\n",
    "        xbatch = torch.tensor(data['start_points'], dtype=x.dtype, device=x.device)\n",
    "        xendbatch = torch.tensor(data['end_points'], dtype=x.dtype, device=x.device)\n",
    "        # xbatch = model.encoder.preprocessor.normalize(xbatch)\n",
    "        # xendbatch = model.encoder.preprocessor.normalize(xendbatch)\n",
    "        # if cfg_main.overfit:\n",
    "        #     ids = torch.eye(xbatch.size(0))\n",
    "        # else:\n",
    "        ids = torch.zeros((xbatch.size(0),1))\n",
    "        dataset = TensorDataset(xbatch, xendbatch, ids)\n",
    "        dataloader = DataLoader(dataset, batch_size=len(x), shuffle=False)\n",
    "\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # def func(x):\n",
    "        #     return model.encoder(x)\n",
    "        # for param in discriminator.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # def discriminator_func_for_grad(x):\n",
    "        #     return discriminator.positive_proba(x, normalize=False).reshape(-1,1)\n",
    "        # def discriminator_func(x):\n",
    "        #     return discriminator.positive_proba(x, normalize=False).reshape(-1,1)\n",
    "        ofm = lambda x: x\n",
    "\n",
    "        checkpoint = torch.load(ckpt_path)\n",
    "\n",
    "        # Remove the \"data_pts\" key from the state_dict\n",
    "        if \"data_pts\" in checkpoint['state_dict']:\n",
    "            del checkpoint['state_dict']['data_pts']\n",
    "\n",
    "        gbmodel = GeodesicBridgeOverfit(\n",
    "            func=ofm,\n",
    "            # func = enc_func,\n",
    "            # discriminator_func=disc_func_pen,\n",
    "            # discriminator_func_for_grad=discriminator_func_for_grad,\n",
    "            input_dim=x.size(1), \n",
    "            hidden_dim=64, \n",
    "            scale_factor=1, \n",
    "            symmetric=True, \n",
    "            num_layers=3, \n",
    "            n_tsteps=100, \n",
    "            lr=1e-3, \n",
    "            weight_decay=1e-3,\n",
    "            discriminator_weight=0.,\n",
    "            discriminator_func_for_grad_weight=0.,\n",
    "            id_dim=1,\n",
    "            id_emb_dim=1,\n",
    "            density_weight=0.,\n",
    "            length_weight=1.,\n",
    "        )\n",
    "\n",
    "        gbmodel.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "        # batch = next(iter(dataloader))\n",
    "        # x0, x1, ids = batch\n",
    "        try:\n",
    "            data_gt = np.load(f\"/gpfs/gibbs/pi/krishnaswamy_smita/xingzhi/dmae/data/neurips_results/toy/gt/{data_name}.npz\", allow_pickle=True)\n",
    "        except:\n",
    "            print(f\"CANNOT FIND FILE {data_name}.npz\")\n",
    "            missing.append(data_name)\n",
    "            continue\n",
    "        xbatch = torch.tensor(data_gt['start_points'], dtype=x.dtype, device=x.device)\n",
    "        xendbatch = torch.tensor(data_gt['end_points'], dtype=x.dtype, device=x.device)\n",
    "        x0 = xbatch\n",
    "        x1 = xendbatch\n",
    "        # xbatch = model.encoder.preprocessor.normalize(xbatch)\n",
    "        # xendbatch = model.encoder.preprocessor.normalize(xendbatch)\n",
    "        ids = torch.zeros((xbatch.size(0),1))\n",
    "        # ids = torch.eye((xbatch.size(0)))\n",
    "\n",
    "        # dataset = TensorDataset(xbatch, xendbatch, ids)\n",
    "        # dataloader = DataLoader(dataset, batch_size=len(z), shuffle=True)\n",
    "\n",
    "        def cc_func(x0, x1, t):\n",
    "            return gbmodel.cc(x0, x1, t, ids)\n",
    "        vectors = velocity(cc_func, gbmodel.ts, x0, x1)\n",
    "        cc_pts = gbmodel.cc(x0, x1, gbmodel.ts, ids)\n",
    "        vectors_flat = vectors.flatten(0,1)\n",
    "        cc_pts_flat = cc_pts.flatten(0, 1)\n",
    "        jac_flat = jacobian(gbmodel.func, cc_pts_flat)\n",
    "        length_all = torch.sqrt((torch.einsum(\"nij,nj->ni\", jac_flat, vectors_flat)**2).sum(axis=1))\n",
    "        length_all = length_all.reshape(vectors.shape[0], vectors.shape[1])\n",
    "        length = length_all.mean(axis=0)\n",
    "\n",
    "        geods = (cc_pts_flat).reshape(cc_pts.shape)\n",
    "        length2 = torch.sqrt(torch.diff(geods, axis=0)**2).sum(axis=-1).sum(axis=0)\n",
    "\n",
    "\n",
    "        # plt.scatter(length2.detach().numpy(), data_gt['geodesic_lengths'])\n",
    "        # plt.title(data_name)\n",
    "        # plt.show()\n",
    "\n",
    "        # gt_len = torch.tensor(data_gt['geodesic_lengths'])\n",
    "        true_geod = torch.tensor(data_gt['geodesics'], dtype=torch.float32)\n",
    "        gt_len = torch.sqrt(torch.diff(true_geod.permute(1,0,2), axis=0)**2).sum(axis=-1).sum(axis=0)\n",
    "        corr = np.corrcoef(length.detach().numpy(), gt_len.cpu().numpy())[0,1]\n",
    "        mse = ((length.detach().numpy() - gt_len.cpu().numpy())**2).mean()\n",
    "        dist2geod = distance_to_geodesic_criterion_len(geods.permute(1,0,2), true_geod, lengths=gt_len).detach().numpy()\n",
    "\n",
    "        res_list.append(dict(\n",
    "            name=data_name,\n",
    "            length_corr=corr,\n",
    "            length_mse=mse,\n",
    "            dist2geod=dist2geod\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        failed.append(data_name)\n",
    "        print(e)\n",
    "\n",
    "res_df = pd.DataFrame(res_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['torus_15_0.7',\n",
       " 'torus_15_0.1',\n",
       " 'torus_50_0.1',\n",
       " 'torus_15_0.5',\n",
       " 'saddle_15_0.1',\n",
       " 'saddle_15_0',\n",
       " 'saddle_10_0.1',\n",
       " 'saddle_50_0.1',\n",
       " 'hemisphere_15_0.3',\n",
       " 'ellipsoid_15_0.5']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]/gpfs/gibbs/pi/krishnaswamy_smita/xingzhi/.conda_envs/geosink/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:269: UserWarning: Attribute 'preprocessor' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['preprocessor'])`.\n",
      "  rank_zero_warn(\n",
      "  3%|▎         | 1/36 [00:00<00:07,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) torus_5_0.1\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 5/36 [00:00<00:03, 10.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) torus_none_0.1\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) torus_15_0.7\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) torus_15_0.1\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) torus_50_0.1\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) torus_15_0.3\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 11/36 [00:00<00:01, 16.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) torus_15_0.5\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) torus_10_0.1\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) saddle_15_0.1\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) saddle_15_0.7\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) saddle_15_0\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) torus_15_0\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) saddle_15_0.5\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 17/36 [00:01<00:01, 14.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) in loading state_dict for GeodesicBridgeOverfit:\n",
      "\tUnexpected key(s) in state_dict: \"data_pts\". \n",
      "Geod not found Series([], Name: run_id, dtype: object) hemisphere_5_0.1\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) hemisphere_50_0.1\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) saddle_none_0.1\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) saddle_15_0.3\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 22/36 [00:01<00:00, 14.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error(s) in loading state_dict for GeodesicBridgeOverfit:\n",
      "\tUnexpected key(s) in state_dict: \"data_pts\". \n",
      "Geod not found Series([], Name: run_id, dtype: object) saddle_5_0.1\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) hemisphere_10_0.1\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) hemisphere_15_0.3\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) hemisphere_15_0.7\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 28/36 [00:02<00:00, 17.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) hemisphere_15_0\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) ellipsoid_15_0.5\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) hemisphere_15_0.5\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) ellipsoid_10_0.1\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) ellipsoid_5_0.1\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 31/36 [00:02<00:00, 18.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) ellipsoid_15_0.7\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) ellipsoid_none_0.1\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) hemisphere_15_0.1\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) hemisphere_none_0.1\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) ellipsoid_50_0.1\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:02<00:00, 15.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geod not found Series([], Name: run_id, dtype: object) ellipsoid_15_0.1\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) ellipsoid_15_0\n",
      "single positional indexer is out-of-bounds\n",
      "Geod not found Series([], Name: run_id, dtype: object) ellipsoid_15_0.3\n",
      "single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res_list = []\n",
    "missing = []\n",
    "failed2 = []\n",
    "\n",
    "for data_name in tqdm(failed):\n",
    "    try:\n",
    "        entity = \"xingzhis\"\n",
    "        project = \"dmae\"\n",
    "        sweep_id = 'ys48kno0'\n",
    "        sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "\n",
    "        runs_data = []\n",
    "\n",
    "        # Iterate through each run in the sweep\n",
    "        for run in sweep.runs:\n",
    "            # Extract metrics and configs\n",
    "            metrics = run.summary._json_dict\n",
    "            configs = run.config\n",
    "            \n",
    "            # Combine metrics and configs, and add run ID\n",
    "            combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "            # Append the combined data to the list\n",
    "            runs_data.append(combined_data)\n",
    "\n",
    "        # Create a DataFrame from the runs data\n",
    "        df = pd.DataFrame(runs_data)\n",
    "\n",
    "        # run_ids = df[(df['data.name'] == data_name) & (df['cfg/loss/weights/cycle'] == 1.) & (df['cfg/dimensions/latent'] == 3)]['run_id']\n",
    "        run_ids = df[(df['data.name'] == data_name) & (df['loss.weights.cycle'] == 1.0) & (df['dimensions.latent'] == 3)]['run_id']\n",
    "        # assert len(run_ids) == 1\n",
    "        if len(run_ids) != 1:\n",
    "            print('AE not found:', run_ids, data_name)\n",
    "        run_id = run_ids.iloc[0]\n",
    "        run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        # run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        folder_path = '../../src/wandb/'\n",
    "        cfg = OmegaConf.create(run.config)\n",
    "        folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        ckpt_path = ckpt_files[0]\n",
    "        cfg.data.root = '../' + cfg.data.root\n",
    "        model = Autoencoder.load_from_checkpoint(ckpt_path)\n",
    "        data = np.load(f\"{cfg.data.root}/{cfg.data.name}{cfg.data.filetype}\", allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "        # sweep_id = 'ywep3ixr'\n",
    "        # sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "        # # Initialize an empty list to store run data\n",
    "        # runs_data = []\n",
    "\n",
    "        # # Iterate through each run in the sweep\n",
    "        # for run in sweep.runs:\n",
    "        #     # Extract metrics and configs\n",
    "        #     metrics = run.summary._json_dict\n",
    "        #     configs = run.config\n",
    "            \n",
    "        #     # Combine metrics and configs, and add run ID\n",
    "        #     combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "        #     # Append the combined data to the list\n",
    "        #     runs_data.append(combined_data)\n",
    "\n",
    "        # # Create a DataFrame from the runs data\n",
    "        # df = pd.DataFrame(runs_data)\n",
    "        # run_ids = df[(df['data.name'] == data_name)][['run_id']]\n",
    "        # assert len(run_ids) == 1\n",
    "        # run_id = run_ids.iloc[0]\n",
    "        # run = api.run(f\"{entity}/{project}/{run_ids.iloc[0].values[0]}\")\n",
    "        # folder_path = '../../src/wandb/'\n",
    "        # cfg = OmegaConf.create(run.config)\n",
    "        # folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        # ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        # ckpt_path = ckpt_files[0]\n",
    "        # cfg.data.root = '../' + cfg.data.root\n",
    "        # discriminator = Discriminator.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "\n",
    "\n",
    "        entity = \"xingzhis\"\n",
    "        project = \"dmae\"\n",
    "        # sweep_id = '88x8glfh'\n",
    "        sweep_id = 'jenfsv5r'\n",
    "        sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "        runs_data = []\n",
    "\n",
    "        # Iterate through each run in the sweep\n",
    "        for run in sweep.runs:\n",
    "            # Extract metrics and configs\n",
    "            metrics = run.summary._json_dict\n",
    "            configs = run.config\n",
    "            \n",
    "            # Combine metrics and configs, and add run ID\n",
    "            combined_data = {**metrics, **configs, \"run_id\": run.id}\n",
    "            \n",
    "            # Append the combined data to the list\n",
    "            runs_data.append(combined_data)\n",
    "\n",
    "        # Create a DataFrame from the runs data\n",
    "        df = pd.DataFrame(runs_data)\n",
    "\n",
    "        run_ids = df[(df['data_name'] == data_name) & (df['loss_epoch'] != 'NaN') & (df['dimensions_latent'] == 3)]['run_id']\n",
    "        # assert len(run_ids) == 1\n",
    "        if len(run_ids) != 1:\n",
    "            print('Geod not found', run_ids, data_name)\n",
    "        run_id = run_ids.iloc[0]\n",
    "        run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "        cfg_main = OmegaConf.create(run.config)\n",
    "        folder_path = '../geodesic_on_datasets//wandb/'\n",
    "        folder_list = glob.glob(f\"{folder_path}*{run.id}*\")\n",
    "        ckpt_files = glob.glob(f\"{folder_list[0]}/files/*.ckpt\")\n",
    "        ckpt_path = ckpt_files[0]\n",
    "\n",
    "        x = torch.tensor(data['data'], dtype=torch.float32, device=model.device)\n",
    "        xbatch = torch.tensor(data['start_points'], dtype=x.dtype, device=x.device)\n",
    "        xendbatch = torch.tensor(data['end_points'], dtype=x.dtype, device=x.device)\n",
    "        # xbatch = model.encoder.preprocessor.normalize(xbatch)\n",
    "        # xendbatch = model.encoder.preprocessor.normalize(xendbatch)\n",
    "        # if cfg_main.overfit:\n",
    "        #     ids = torch.eye(xbatch.size(0))\n",
    "        # else:\n",
    "        ids = torch.zeros((xbatch.size(0),1))\n",
    "        dataset = TensorDataset(xbatch, xendbatch, ids)\n",
    "        dataloader = DataLoader(dataset, batch_size=len(x), shuffle=False)\n",
    "\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # def func(x):\n",
    "        #     return model.encoder(x)\n",
    "        # for param in discriminator.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # def discriminator_func_for_grad(x):\n",
    "        #     return discriminator.positive_proba(x, normalize=False).reshape(-1,1)\n",
    "        # def discriminator_func(x):\n",
    "        #     return discriminator.positive_proba(x, normalize=False).reshape(-1,1)\n",
    "        ofm = lambda x: x\n",
    "        gbmodel = GeodesicBridgeOverfit.load_from_checkpoint(\n",
    "            checkpoint_path=ckpt_path,\n",
    "            func=ofm,\n",
    "            # func = enc_func,\n",
    "            # discriminator_func=disc_func_pen,\n",
    "            # discriminator_func_for_grad=discriminator_func_for_grad,\n",
    "            input_dim=x.size(1), \n",
    "            hidden_dim=64, \n",
    "            scale_factor=1, \n",
    "            symmetric=True, \n",
    "            num_layers=3, \n",
    "            n_tsteps=100, \n",
    "            lr=1e-3, \n",
    "            weight_decay=1e-3,\n",
    "            discriminator_weight=0.,\n",
    "            discriminator_func_for_grad_weight=0.,\n",
    "            id_dim=1,\n",
    "            id_emb_dim=1,\n",
    "            density_weight=0.,\n",
    "            length_weight=1.,\n",
    "        )\n",
    "\n",
    "        # batch = next(iter(dataloader))\n",
    "        # x0, x1, ids = batch\n",
    "        try:\n",
    "            data_gt = np.load(f\"/gpfs/gibbs/pi/krishnaswamy_smita/xingzhi/dmae/data/neurips_results/toy/gt/{data_name}.npz\", allow_pickle=True)\n",
    "        except:\n",
    "            print(f\"CANNOT FIND FILE {data_name}.npz\")\n",
    "            missing.append(data_name)\n",
    "            continue\n",
    "        xbatch = torch.tensor(data_gt['start_points'], dtype=x.dtype, device=x.device)\n",
    "        xendbatch = torch.tensor(data_gt['end_points'], dtype=x.dtype, device=x.device)\n",
    "        x0 = xbatch\n",
    "        x1 = xendbatch\n",
    "        # xbatch = model.encoder.preprocessor.normalize(xbatch)\n",
    "        # xendbatch = model.encoder.preprocessor.normalize(xendbatch)\n",
    "        ids = torch.zeros((xbatch.size(0),1))\n",
    "        # ids = torch.eye((xbatch.size(0)))\n",
    "\n",
    "        # dataset = TensorDataset(xbatch, xendbatch, ids)\n",
    "        # dataloader = DataLoader(dataset, batch_size=len(z), shuffle=True)\n",
    "\n",
    "        def cc_func(x0, x1, t):\n",
    "            return gbmodel.cc(x0, x1, t, ids)\n",
    "        vectors = velocity(cc_func, gbmodel.ts, x0, x1)\n",
    "        cc_pts = gbmodel.cc(x0, x1, gbmodel.ts, ids)\n",
    "        vectors_flat = vectors.flatten(0,1)\n",
    "        cc_pts_flat = cc_pts.flatten(0, 1)\n",
    "        jac_flat = jacobian(gbmodel.func, cc_pts_flat)\n",
    "        length_all = torch.sqrt((torch.einsum(\"nij,nj->ni\", jac_flat, vectors_flat)**2).sum(axis=1))\n",
    "        length_all = length_all.reshape(vectors.shape[0], vectors.shape[1])\n",
    "        length = length_all.mean(axis=0)\n",
    "\n",
    "        geods = (cc_pts_flat).reshape(cc_pts.shape)\n",
    "        length2 = torch.sqrt(torch.diff(geods, axis=0)**2).sum(axis=-1).sum(axis=0)\n",
    "\n",
    "\n",
    "        # plt.scatter(length2.detach().numpy(), data_gt['geodesic_lengths'])\n",
    "        # plt.title(data_name)\n",
    "        # plt.show()\n",
    "\n",
    "        # gt_len = torch.tensor(data_gt['geodesic_lengths'])\n",
    "        true_geod = torch.tensor(data_gt['geodesics'], dtype=torch.float32)\n",
    "        gt_len = torch.sqrt(torch.diff(true_geod.permute(1,0,2), axis=0)**2).sum(axis=-1).sum(axis=0)\n",
    "        corr = np.corrcoef(length.detach().numpy(), gt_len.cpu().numpy())[0,1]\n",
    "        mse = ((length.detach().numpy() - gt_len.cpu().numpy())**2).mean()\n",
    "        dist2geod = distance_to_geodesic_criterion_len(geods.permute(1,0,2), true_geod, lengths=gt_len).detach().numpy()\n",
    "\n",
    "        res_list.append(dict(\n",
    "            name=data_name,\n",
    "            length_corr=corr,\n",
    "            length_mse=mse,\n",
    "            dist2geod=dist2geod\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        failed2.append(data_name)\n",
    "        print(e)\n",
    "\n",
    "res_df2 = pd.DataFrame(res_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ours = pd.concat([res_df, res_df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2095352/379785340.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_ours\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/gpfs/gibbs/pi/krishnaswamy_smita/xingzhi/.conda_envs/geosink/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6754\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6755\u001b[0m             \u001b[0;31m# len(by) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6757\u001b[0m             \u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6758\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6760\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6761\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/gibbs/pi/krishnaswamy_smita/xingzhi/.conda_envs/geosink/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'name'"
     ]
    }
   ],
   "source": [
    "results_ours.sort_values('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>length_corr</th>\n",
       "      <th>length_mse</th>\n",
       "      <th>dist2geod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>torus_5_0.1</td>\n",
       "      <td>0.960356</td>\n",
       "      <td>9.875797</td>\n",
       "      <td>0.053779595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>torus_none_0.1</td>\n",
       "      <td>0.987134</td>\n",
       "      <td>8.515339</td>\n",
       "      <td>0.059096955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>torus_15_0.7</td>\n",
       "      <td>0.478949</td>\n",
       "      <td>14.321295</td>\n",
       "      <td>1.2238166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>torus_50_0.1</td>\n",
       "      <td>0.933761</td>\n",
       "      <td>33.379780</td>\n",
       "      <td>0.105932415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>torus_15_0.3</td>\n",
       "      <td>0.907351</td>\n",
       "      <td>18.993786</td>\n",
       "      <td>0.2710719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>torus_15_0.5</td>\n",
       "      <td>0.821626</td>\n",
       "      <td>16.251606</td>\n",
       "      <td>0.590967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>torus_10_0.1</td>\n",
       "      <td>0.931861</td>\n",
       "      <td>14.914821</td>\n",
       "      <td>0.039224662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>saddle_15_0.1</td>\n",
       "      <td>0.877697</td>\n",
       "      <td>2.965390</td>\n",
       "      <td>0.07588364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>saddle_15_0.7</td>\n",
       "      <td>0.084417</td>\n",
       "      <td>5.793678</td>\n",
       "      <td>2.3582518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>saddle_15_0</td>\n",
       "      <td>0.962471</td>\n",
       "      <td>4.047969</td>\n",
       "      <td>0.019819994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>torus_15_0</td>\n",
       "      <td>0.984406</td>\n",
       "      <td>25.852291</td>\n",
       "      <td>0.029601594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>saddle_15_0.5</td>\n",
       "      <td>0.548730</td>\n",
       "      <td>1.696799</td>\n",
       "      <td>1.2525997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hemisphere_5_0.1</td>\n",
       "      <td>0.957537</td>\n",
       "      <td>1.130601</td>\n",
       "      <td>0.024962643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hemisphere_50_0.1</td>\n",
       "      <td>0.758552</td>\n",
       "      <td>3.916905</td>\n",
       "      <td>0.2687152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>saddle_none_0.1</td>\n",
       "      <td>0.959963</td>\n",
       "      <td>0.950617</td>\n",
       "      <td>0.0140607115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>saddle_5_0.1</td>\n",
       "      <td>0.934310</td>\n",
       "      <td>1.502157</td>\n",
       "      <td>0.019828657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hemisphere_10_0.1</td>\n",
       "      <td>0.878363</td>\n",
       "      <td>2.434143</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hemisphere_15_0.7</td>\n",
       "      <td>0.087843</td>\n",
       "      <td>5.381284</td>\n",
       "      <td>2.7860541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hemisphere_15_0</td>\n",
       "      <td>0.987028</td>\n",
       "      <td>1.848050</td>\n",
       "      <td>0.0029537065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ellipsoid_15_0.5</td>\n",
       "      <td>0.605532</td>\n",
       "      <td>9.252222</td>\n",
       "      <td>0.5361252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hemisphere_15_0.5</td>\n",
       "      <td>0.158976</td>\n",
       "      <td>1.654370</td>\n",
       "      <td>1.2332762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ellipsoid_10_0.1</td>\n",
       "      <td>0.964231</td>\n",
       "      <td>9.102427</td>\n",
       "      <td>0.060074233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ellipsoid_5_0.1</td>\n",
       "      <td>0.980016</td>\n",
       "      <td>6.260004</td>\n",
       "      <td>0.017300833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ellipsoid_15_0.7</td>\n",
       "      <td>0.008080</td>\n",
       "      <td>4.497561</td>\n",
       "      <td>1.1688144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ellipsoid_none_0.1</td>\n",
       "      <td>0.970372</td>\n",
       "      <td>5.426956</td>\n",
       "      <td>0.03317458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hemisphere_15_0.1</td>\n",
       "      <td>0.941173</td>\n",
       "      <td>2.950792</td>\n",
       "      <td>0.07492655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>hemisphere_none_0.1</td>\n",
       "      <td>0.887544</td>\n",
       "      <td>0.787965</td>\n",
       "      <td>0.03146708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ellipsoid_50_0.1</td>\n",
       "      <td>0.873398</td>\n",
       "      <td>17.298132</td>\n",
       "      <td>0.15254375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ellipsoid_15_0.1</td>\n",
       "      <td>0.857788</td>\n",
       "      <td>12.115648</td>\n",
       "      <td>0.055831768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ellipsoid_15_0</td>\n",
       "      <td>0.976680</td>\n",
       "      <td>13.345522</td>\n",
       "      <td>0.0135096805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ellipsoid_15_0.3</td>\n",
       "      <td>0.724161</td>\n",
       "      <td>14.871043</td>\n",
       "      <td>0.21918336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>saddle_10_0.1</td>\n",
       "      <td>0.960217</td>\n",
       "      <td>1.611827</td>\n",
       "      <td>0.050164558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>saddle_50_0.1</td>\n",
       "      <td>0.924417</td>\n",
       "      <td>3.610201</td>\n",
       "      <td>0.16434935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  length_corr  length_mse     dist2geod\n",
       "0           torus_5_0.1     0.960356    9.875797   0.053779595\n",
       "1        torus_none_0.1     0.987134    8.515339   0.059096955\n",
       "2          torus_15_0.7     0.478949   14.321295     1.2238166\n",
       "3          torus_50_0.1     0.933761   33.379780   0.105932415\n",
       "4          torus_15_0.3     0.907351   18.993786     0.2710719\n",
       "5          torus_15_0.5     0.821626   16.251606      0.590967\n",
       "6          torus_10_0.1     0.931861   14.914821   0.039224662\n",
       "7         saddle_15_0.1     0.877697    2.965390    0.07588364\n",
       "8         saddle_15_0.7     0.084417    5.793678     2.3582518\n",
       "9           saddle_15_0     0.962471    4.047969   0.019819994\n",
       "10           torus_15_0     0.984406   25.852291   0.029601594\n",
       "11        saddle_15_0.5     0.548730    1.696799     1.2525997\n",
       "12     hemisphere_5_0.1     0.957537    1.130601   0.024962643\n",
       "13    hemisphere_50_0.1     0.758552    3.916905     0.2687152\n",
       "14      saddle_none_0.1     0.959963    0.950617  0.0140607115\n",
       "15         saddle_5_0.1     0.934310    1.502157   0.019828657\n",
       "16    hemisphere_10_0.1     0.878363    2.434143           inf\n",
       "17    hemisphere_15_0.7     0.087843    5.381284     2.7860541\n",
       "18      hemisphere_15_0     0.987028    1.848050  0.0029537065\n",
       "19     ellipsoid_15_0.5     0.605532    9.252222     0.5361252\n",
       "20    hemisphere_15_0.5     0.158976    1.654370     1.2332762\n",
       "21     ellipsoid_10_0.1     0.964231    9.102427   0.060074233\n",
       "22      ellipsoid_5_0.1     0.980016    6.260004   0.017300833\n",
       "23     ellipsoid_15_0.7     0.008080    4.497561     1.1688144\n",
       "24   ellipsoid_none_0.1     0.970372    5.426956    0.03317458\n",
       "25    hemisphere_15_0.1     0.941173    2.950792    0.07492655\n",
       "26  hemisphere_none_0.1     0.887544    0.787965    0.03146708\n",
       "27     ellipsoid_50_0.1     0.873398   17.298132    0.15254375\n",
       "28     ellipsoid_15_0.1     0.857788   12.115648   0.055831768\n",
       "29       ellipsoid_15_0     0.976680   13.345522  0.0135096805\n",
       "30     ellipsoid_15_0.3     0.724161   14.871043    0.21918336\n",
       "31        saddle_10_0.1     0.960217    1.611827   0.050164558\n",
       "32        saddle_50_0.1     0.924417    3.610201    0.16434935"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ours.to_csv('geodesics_density.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['torus_15_0.1', 'saddle_15_0.3', 'hemisphere_15_0.3']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "failed2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
