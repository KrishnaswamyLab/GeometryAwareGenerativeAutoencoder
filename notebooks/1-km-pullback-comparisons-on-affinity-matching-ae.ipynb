{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "from fastcore.all import *\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "# Configure environment\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE']='false' # Tells Jax not to hog all of the memory to this process.\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "# set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src/')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pullback Comparisons with the Affinity Matching AE\n",
    "> Witty encapsulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis**: Stuff will happen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machinery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autometric.datasets import Hemisphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere = Hemisphere(num_points = 2000, r = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.affinity_matching import AffinityMatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hypers = {\n",
    "    'ambient_dimension': 3,\n",
    "    'latent_dimension': 2,\n",
    "    'model_type': 'affinity',\n",
    "    'loss_type': 'kl',\n",
    "    'activation': 'relu',\n",
    "    'layer_widths': [256, 128, 64],\n",
    "    'kernel_method': 'gaussian',\n",
    "    'kernel_alpha': 1,\n",
    "    'kernel_bandwidth': 1,\n",
    "    'knn': 5,\n",
    "    't': 0,\n",
    "    'n_landmark': 5000,\n",
    "    'verbose': False\n",
    "}\n",
    "training_hypers = {\n",
    "    'data_name': 'randomtest',\n",
    "    'max_epochs': 100,\n",
    "    'batch_size': 64,\n",
    "    'lr': 1e-3,\n",
    "    'shuffle': True,\n",
    "    'weight_decay': 1e-5,\n",
    "    'monitor': 'val_loss',\n",
    "    'patience': 100,\n",
    "    'seed': 2024,\n",
    "    'log_every_n_steps': 100,\n",
    "    'accelerator': 'auto',\n",
    "    'train_from_scratch': True,\n",
    "    'model_save_path': './affinity_matching'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hemisphere.X.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PHATE on 983 observations and 3 variables.\n",
      "Calculating graph and diffusion operator...\n",
      "  Calculating KNN search...\n",
      "  Calculating affinities...\n",
      "Calculating optimal t...\n",
      "  Automatically selected t = 27\n",
      "Calculated optimal t in 0.12 seconds.\n",
      "Calculating diffusion potential...\n",
      "Calculated diffusion potential in 0.03 seconds.\n",
      "Calculating metric MDS...\n",
      "Calculated metric MDS in 0.36 seconds.\n",
      "row_stochastic_matrix torch.Size([983, 983])\n",
      "checking row sum: False\n",
      "row sum:  tensor([1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001])\n",
      "Calculating optimal t...\n",
      "  Automatically selected t = 27\n",
      "Calculated optimal t in 0.12 seconds.\n",
      "Running PHATE on 1405 observations and 3 variables.\n",
      "Calculating graph and diffusion operator...\n",
      "  Calculating KNN search...\n",
      "  Calculating affinities...\n",
      "Calculated graph and diffusion operator in 0.01 seconds.\n",
      "Calculating optimal t...\n",
      "  Automatically selected t = 23\n",
      "Calculated optimal t in 0.24 seconds.\n",
      "Calculating diffusion potential...\n",
      "Calculated diffusion potential in 0.08 seconds.\n",
      "Calculating metric MDS...\n",
      "Calculated metric MDS in 0.85 seconds.\n",
      "row_stochastic_matrix torch.Size([1405, 1405])\n",
      "checking row sum: False\n",
      "row sum:  tensor([1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001])\n",
      "Calculating optimal t...\n",
      "  Automatically selected t = 23\n",
      "Calculated optimal t in 0.23 seconds.\n",
      "Running PHATE on 2008 observations and 3 variables.\n",
      "Calculating graph and diffusion operator...\n",
      "  Calculating KNN search...\n",
      "  Calculating affinities...\n",
      "Calculated graph and diffusion operator in 0.02 seconds.\n",
      "Calculating optimal t...\n",
      "  Automatically selected t = 23\n",
      "Calculated optimal t in 0.54 seconds.\n",
      "Calculating diffusion potential...\n",
      "Calculated diffusion potential in 0.23 seconds.\n",
      "Calculating metric MDS...\n",
      "Calculated metric MDS in 2.82 seconds.\n",
      "row_stochastic_matrix torch.Size([2008, 2008])\n",
      "checking row sum: False\n",
      "row sum:  tensor([1.0002, 1.0002, 1.0002, 1.0002, 1.0002, 1.0002, 1.0002, 1.0002, 1.0002,\n",
      "        1.0002, 1.0002, 1.0002, 1.0002, 1.0002, 1.0002, 1.0002, 1.0002, 1.0002,\n",
      "        1.0002, 1.0002])\n",
      "Calculating optimal t...\n",
      "  Automatically selected t = 23\n",
      "Calculated optimal t in 0.56 seconds.\n",
      "Train dataset: 983;           Val dataset: 1405;           Whole dataset: 2008\n",
      "Training Encoder ...\n",
      "[Epoch: 0]: Encoder Loss: 2.3935492038726807\n",
      "\n",
      "[Epoch: 0]: Val Encoder Loss: 2.868333578109741\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 1]: Val Encoder Loss: 2.8368780612945557\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 2]: Val Encoder Loss: 2.801698684692383\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 3]: Val Encoder Loss: 2.7622058391571045\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 4]: Val Encoder Loss: 2.7185420989990234\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 5]: Val Encoder Loss: 2.6712968349456787\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 6]: Val Encoder Loss: 2.6204042434692383\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 7]: Val Encoder Loss: 2.565833806991577\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 8]: Val Encoder Loss: 2.507077217102051\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 9]: Val Encoder Loss: 2.443673610687256\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 10]: Val Encoder Loss: 2.3752989768981934\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 11]: Val Encoder Loss: 2.302065849304199\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 12]: Val Encoder Loss: 2.2241528034210205\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 13]: Val Encoder Loss: 2.141899824142456\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 14]: Val Encoder Loss: 2.0557656288146973\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 15]: Val Encoder Loss: 1.9665131568908691\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 16]: Val Encoder Loss: 1.8748310804367065\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 17]: Val Encoder Loss: 1.7814674377441406\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 18]: Val Encoder Loss: 1.6873130798339844\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 19]: Val Encoder Loss: 1.5931860208511353\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 20]: Val Encoder Loss: 1.49995756149292\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 21]: Val Encoder Loss: 1.4084815979003906\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 22]: Val Encoder Loss: 1.319488286972046\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 23]: Val Encoder Loss: 1.2335819005966187\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 24]: Val Encoder Loss: 1.1512757539749146\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 25]: Val Encoder Loss: 1.073015809059143\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 26]: Val Encoder Loss: 0.9991595149040222\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 27]: Val Encoder Loss: 0.929828941822052\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 28]: Val Encoder Loss: 0.8650465607643127\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 29]: Val Encoder Loss: 0.8047251105308533\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 30]: Val Encoder Loss: 0.7487210035324097\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 31]: Val Encoder Loss: 0.6968785524368286\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 32]: Val Encoder Loss: 0.6490898728370667\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 33]: Val Encoder Loss: 0.6053943634033203\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 34]: Val Encoder Loss: 0.5658400654792786\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 35]: Val Encoder Loss: 0.5301695466041565\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 36]: Val Encoder Loss: 0.4981672167778015\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 37]: Val Encoder Loss: 0.4694494903087616\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 38]: Val Encoder Loss: 0.443708211183548\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 39]: Val Encoder Loss: 0.42059555649757385\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 40]: Val Encoder Loss: 0.39981788396835327\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 41]: Val Encoder Loss: 0.3811500072479248\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 42]: Val Encoder Loss: 0.3644751012325287\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 43]: Val Encoder Loss: 0.3497560918331146\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 44]: Val Encoder Loss: 0.3367631435394287\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 45]: Val Encoder Loss: 0.325358122587204\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 46]: Val Encoder Loss: 0.315408319234848\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 47]: Val Encoder Loss: 0.30682164430618286\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 48]: Val Encoder Loss: 0.29946771264076233\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 49]: Val Encoder Loss: 0.29319190979003906\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 50]: Val Encoder Loss: 0.28801706433296204\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 51]: Val Encoder Loss: 0.2837532162666321\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 52]: Val Encoder Loss: 0.28033962845802307\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 53]: Val Encoder Loss: 0.27778542041778564\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 54]: Val Encoder Loss: 0.2760252356529236\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 55]: Val Encoder Loss: 0.2749451696872711\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 56]: Val Encoder Loss: 0.27446067333221436\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 57]: Val Encoder Loss: 0.27452340722084045\n",
      "\n",
      "[Epoch: 58]: Val Encoder Loss: 0.27504047751426697\n",
      "\n",
      "[Epoch: 59]: Val Encoder Loss: 0.2758680582046509\n",
      "\n",
      "[Epoch: 60]: Val Encoder Loss: 0.2768498361110687\n",
      "\n",
      "[Epoch: 61]: Val Encoder Loss: 0.2778955399990082\n",
      "\n",
      "[Epoch: 62]: Val Encoder Loss: 0.2789100408554077\n",
      "\n",
      "[Epoch: 63]: Val Encoder Loss: 0.2797769010066986\n",
      "\n",
      "[Epoch: 64]: Val Encoder Loss: 0.280383437871933\n",
      "\n",
      "[Epoch: 65]: Val Encoder Loss: 0.28065434098243713\n",
      "\n",
      "[Epoch: 66]: Val Encoder Loss: 0.2805454432964325\n",
      "\n",
      "[Epoch: 67]: Val Encoder Loss: 0.28004881739616394\n",
      "\n",
      "[Epoch: 68]: Val Encoder Loss: 0.27918344736099243\n",
      "\n",
      "[Epoch: 69]: Val Encoder Loss: 0.27798759937286377\n",
      "\n",
      "[Epoch: 70]: Val Encoder Loss: 0.2764874994754791\n",
      "\n",
      "[Epoch: 71]: Val Encoder Loss: 0.27474936842918396\n",
      "\n",
      "[Epoch: 72]: Val Encoder Loss: 0.27284741401672363\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 73]: Val Encoder Loss: 0.270829975605011\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 74]: Val Encoder Loss: 0.2688145637512207\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 75]: Val Encoder Loss: 0.266865074634552\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 76]: Val Encoder Loss: 0.26504865288734436\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 77]: Val Encoder Loss: 0.26333802938461304\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 78]: Val Encoder Loss: 0.2617954611778259\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 79]: Val Encoder Loss: 0.26047319173812866\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 80]: Val Encoder Loss: 0.25942665338516235\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 81]: Val Encoder Loss: 0.25858503580093384\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 82]: Val Encoder Loss: 0.2579922378063202\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 83]: Val Encoder Loss: 0.25768348574638367\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 84]: Val Encoder Loss: 0.2575204372406006\n",
      "\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "\n",
      "[Epoch: 85]: Val Encoder Loss: 0.25760599970817566\n",
      "\n",
      "[Epoch: 86]: Val Encoder Loss: 0.25789180397987366\n",
      "\n",
      "[Epoch: 87]: Val Encoder Loss: 0.2583398222923279\n",
      "\n",
      "[Epoch: 88]: Val Encoder Loss: 0.25892090797424316\n",
      "\n",
      "[Epoch: 89]: Val Encoder Loss: 0.25959664583206177\n",
      "\n",
      "[Epoch: 90]: Val Encoder Loss: 0.2603127658367157\n",
      "\n",
      "[Epoch: 91]: Val Encoder Loss: 0.26105308532714844\n",
      "\n",
      "[Epoch: 92]: Val Encoder Loss: 0.261761337518692\n",
      "\n",
      "[Epoch: 93]: Val Encoder Loss: 0.26229536533355713\n",
      "\n",
      "[Epoch: 94]: Val Encoder Loss: 0.26269131898880005\n",
      "\n",
      "[Epoch: 95]: Val Encoder Loss: 0.2629716396331787\n",
      "\n",
      "[Epoch: 96]: Val Encoder Loss: 0.2631423771381378\n",
      "\n",
      "[Epoch: 97]: Val Encoder Loss: 0.2633040249347687\n",
      "\n",
      "[Epoch: 98]: Val Encoder Loss: 0.2634042799472809\n",
      "\n",
      "[Epoch: 99]: Val Encoder Loss: 0.26339396834373474\n",
      "Done training encoder.\n",
      "[Epoch: 0]: Decoder Loss: 0.12983988714404404\n",
      "[Epoch: 0]: Val Decoder Loss: 0.06395568326115608\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 1]: Val Decoder Loss: 0.031979276931711605\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 2]: Val Decoder Loss: 0.009263877091663224\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 3]: Val Decoder Loss: 0.005433468015066215\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 4]: Val Decoder Loss: 0.0045385408281747785\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 5]: Val Decoder Loss: 0.00420355933186199\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 6]: Val Decoder Loss: 0.004198718616472823\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 7]: Val Decoder Loss: 0.003791406850463578\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 8]: Val Decoder Loss: 0.002469056086348636\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 9]: Val Decoder Loss: 0.0012138629515123154\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 10]: Val Decoder Loss: 0.0008965336684403675\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 11]: Val Decoder Loss: 0.0008551168388554028\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 12]: Val Decoder Loss: 0.0009991518704087607\n",
      "Done training decoder.\n",
      "[Epoch: 13]: Val Decoder Loss: 0.0011664323641785554\n",
      "Done training decoder.\n",
      "[Epoch: 14]: Val Decoder Loss: 0.0012308592309377023\n",
      "Done training decoder.\n",
      "[Epoch: 15]: Val Decoder Loss: 0.0010791125491128436\n",
      "Done training decoder.\n",
      "[Epoch: 16]: Val Decoder Loss: 0.0009064916438157005\n",
      "Done training decoder.\n",
      "[Epoch: 17]: Val Decoder Loss: 0.0007218455263812627\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 18]: Val Decoder Loss: 0.0004907640478839832\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 19]: Val Decoder Loss: 0.0003796279862789171\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 20]: Val Decoder Loss: 0.0003138230593841789\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 21]: Val Decoder Loss: 0.0002761637816937374\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 22]: Val Decoder Loss: 0.00027039271247174056\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 23]: Val Decoder Loss: 0.00023565254066072936\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 24]: Val Decoder Loss: 0.00031584871508779803\n",
      "Done training decoder.\n",
      "[Epoch: 25]: Val Decoder Loss: 0.00023234874866570214\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 26]: Val Decoder Loss: 0.0002296712960482442\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 27]: Val Decoder Loss: 0.00028167471256373186\n",
      "Done training decoder.\n",
      "[Epoch: 28]: Val Decoder Loss: 0.0003523300983943045\n",
      "Done training decoder.\n",
      "[Epoch: 29]: Val Decoder Loss: 0.0003980859390659524\n",
      "Done training decoder.\n",
      "[Epoch: 30]: Val Decoder Loss: 0.0004127315021053489\n",
      "Done training decoder.\n",
      "[Epoch: 31]: Val Decoder Loss: 0.0004171971689044897\n",
      "Done training decoder.\n",
      "[Epoch: 32]: Val Decoder Loss: 0.0004181149207787322\n",
      "Done training decoder.\n",
      "[Epoch: 33]: Val Decoder Loss: 0.0004258000449876168\n",
      "Done training decoder.\n",
      "[Epoch: 34]: Val Decoder Loss: 0.0004094263172841498\n",
      "Done training decoder.\n",
      "[Epoch: 35]: Val Decoder Loss: 0.00040560804440506866\n",
      "Done training decoder.\n",
      "[Epoch: 36]: Val Decoder Loss: 0.0004798741720151156\n",
      "Done training decoder.\n",
      "[Epoch: 37]: Val Decoder Loss: 0.00048688441165722907\n",
      "Done training decoder.\n",
      "[Epoch: 38]: Val Decoder Loss: 0.0005385614474237498\n",
      "Done training decoder.\n",
      "[Epoch: 39]: Val Decoder Loss: 0.0006179787825593459\n",
      "Done training decoder.\n",
      "[Epoch: 40]: Val Decoder Loss: 0.0006776847003493458\n",
      "Done training decoder.\n",
      "[Epoch: 41]: Val Decoder Loss: 0.0008085447646278356\n",
      "Done training decoder.\n",
      "[Epoch: 42]: Val Decoder Loss: 0.000916126841080508\n",
      "Done training decoder.\n",
      "[Epoch: 43]: Val Decoder Loss: 0.0008867064441022064\n",
      "Done training decoder.\n",
      "[Epoch: 44]: Val Decoder Loss: 0.0008674713733073856\n",
      "Done training decoder.\n",
      "[Epoch: 45]: Val Decoder Loss: 0.0005532068898901343\n",
      "Done training decoder.\n",
      "[Epoch: 46]: Val Decoder Loss: 0.0003288499595198248\n",
      "Done training decoder.\n",
      "[Epoch: 47]: Val Decoder Loss: 0.00022369537951557765\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 48]: Val Decoder Loss: 0.0002207808699625145\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 49]: Val Decoder Loss: 0.00021694874158129096\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 50]: Val Decoder Loss: 0.0001962132623053289\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 51]: Val Decoder Loss: 0.00017499955928152694\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 52]: Val Decoder Loss: 0.00015843781043908427\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 53]: Val Decoder Loss: 0.00014235741504567808\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 54]: Val Decoder Loss: 0.00013653044983844405\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 55]: Val Decoder Loss: 0.00012827443528554535\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 56]: Val Decoder Loss: 0.00012093227165418543\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 57]: Val Decoder Loss: 0.0001144601805468223\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 58]: Val Decoder Loss: 0.00010773869040089526\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 59]: Val Decoder Loss: 0.00010354815562355466\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 60]: Val Decoder Loss: 9.656263864599168e-05\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 61]: Val Decoder Loss: 9.906546828070921e-05\n",
      "Done training decoder.\n",
      "[Epoch: 62]: Val Decoder Loss: 9.962546443732987e-05\n",
      "Done training decoder.\n",
      "[Epoch: 63]: Val Decoder Loss: 0.00010399598042048248\n",
      "Done training decoder.\n",
      "[Epoch: 64]: Val Decoder Loss: 0.00010108789865626022\n",
      "Done training decoder.\n",
      "[Epoch: 65]: Val Decoder Loss: 0.00010574334633669682\n",
      "Done training decoder.\n",
      "[Epoch: 66]: Val Decoder Loss: 0.00010120973664535475\n",
      "Done training decoder.\n",
      "[Epoch: 67]: Val Decoder Loss: 0.00011043628702671933\n",
      "Done training decoder.\n",
      "[Epoch: 68]: Val Decoder Loss: 0.00010147396512495886\n",
      "Done training decoder.\n",
      "[Epoch: 69]: Val Decoder Loss: 0.00010190731700276956\n",
      "Done training decoder.\n",
      "[Epoch: 70]: Val Decoder Loss: 0.00010495175437037168\n",
      "Done training decoder.\n",
      "[Epoch: 71]: Val Decoder Loss: 0.00011510883320754926\n",
      "Done training decoder.\n",
      "[Epoch: 72]: Val Decoder Loss: 0.00012804965185101276\n",
      "Done training decoder.\n",
      "[Epoch: 73]: Val Decoder Loss: 0.00014503493418617706\n",
      "Done training decoder.\n",
      "[Epoch: 74]: Val Decoder Loss: 0.00014266012398625856\n",
      "Done training decoder.\n",
      "[Epoch: 75]: Val Decoder Loss: 0.00014157090897372524\n",
      "Done training decoder.\n",
      "[Epoch: 76]: Val Decoder Loss: 0.00013727210482881804\n",
      "Done training decoder.\n",
      "[Epoch: 77]: Val Decoder Loss: 0.00012277478838638802\n",
      "Done training decoder.\n",
      "[Epoch: 78]: Val Decoder Loss: 0.00011445543246476777\n",
      "Done training decoder.\n",
      "[Epoch: 79]: Val Decoder Loss: 9.779920427328242e-05\n",
      "Done training decoder.\n",
      "[Epoch: 80]: Val Decoder Loss: 7.913952296283761e-05\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 81]: Val Decoder Loss: 7.32451730332936e-05\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 82]: Val Decoder Loss: 7.249221432305473e-05\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 83]: Val Decoder Loss: 7.151012011620748e-05\n",
      "Better model found. Saving best model ...\n",
      "\n",
      "Done training decoder.\n",
      "[Epoch: 84]: Val Decoder Loss: 8.053513686588434e-05\n",
      "Done training decoder.\n",
      "[Epoch: 85]: Val Decoder Loss: 8.784217165417172e-05\n",
      "Done training decoder.\n",
      "[Epoch: 86]: Val Decoder Loss: 8.541935104793603e-05\n",
      "Done training decoder.\n",
      "[Epoch: 87]: Val Decoder Loss: 9.893853920012978e-05\n",
      "Done training decoder.\n",
      "[Epoch: 88]: Val Decoder Loss: 0.00012189661044560905\n",
      "Done training decoder.\n",
      "[Epoch: 89]: Val Decoder Loss: 0.00015294705995724404\n",
      "Done training decoder.\n",
      "[Epoch: 90]: Val Decoder Loss: 0.00017602333641012331\n",
      "Done training decoder.\n",
      "[Epoch: 91]: Val Decoder Loss: 0.0001706432698743551\n",
      "Done training decoder.\n",
      "[Epoch: 92]: Val Decoder Loss: 0.00017076332733267918\n",
      "Done training decoder.\n",
      "[Epoch: 93]: Val Decoder Loss: 0.00016148409589992037\n",
      "Done training decoder.\n",
      "[Epoch: 94]: Val Decoder Loss: 0.0001307122001890093\n",
      "Done training decoder.\n",
      "[Epoch: 95]: Val Decoder Loss: 0.0001764388060629634\n",
      "Done training decoder.\n",
      "[Epoch: 96]: Val Decoder Loss: 0.00045798798756940026\n",
      "Done training decoder.\n",
      "[Epoch: 97]: Val Decoder Loss: 0.0010505345749801823\n",
      "Done training decoder.\n",
      "[Epoch: 98]: Val Decoder Loss: 0.0013598532282880374\n",
      "Done training decoder.\n",
      "[Epoch: 99]: Val Decoder Loss: 0.000701812154147774\n",
      "Done training decoder.\n",
      "Done fitting model.\n",
      "Encoded Z: (2008, 2)\n",
      "Decoded X: (2008, 3)\n"
     ]
    }
   ],
   "source": [
    "# Test AffinityMatching model\n",
    "model = AffinityMatching(**model_hypers)\n",
    "model.fit(\n",
    "    X,\n",
    "    train_mask=None, \n",
    "    percent_test=0.3, \n",
    "    **training_hypers)\n",
    "\n",
    "Z = model.encode(X)\n",
    "print('Encoded Z:', Z.shape)\n",
    "X_hat = model.decode(Z)\n",
    "print('Decoded X:', X_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmae-iterant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
