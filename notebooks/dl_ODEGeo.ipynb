{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danqiliao/opt/miniconda3/envs/yale529/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scprep\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src')\n",
    "from models.unified_model import GeometricAE\n",
    "from models.distance_matching import DistanceMatching\n",
    "from data_script import hemisphere_data, sklearn_swiss_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data_name = 'swiss_roll'\n",
    "if data_name == 'swiss_roll':\n",
    "    gt_X, X, _ = sklearn_swiss_roll(n_samples=1000, noise=0.0)\n",
    "    colors = None\n",
    "elif data_name == 'hemisphere':\n",
    "    gt_X, X, _ = hemisphere_data(n_samples=1000, noise=0.0)\n",
    "    colors = None\n",
    "\n",
    "print(gt_X.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'encoder'\n",
    "model_hypers = {\n",
    "    'ambient_dimension': 3,\n",
    "    'latent_dimension': 2,\n",
    "    'model_type': 'distance',\n",
    "    'activation': 'relu',\n",
    "    'layer_widths': [256, 128, 64],\n",
    "    'knn': 10,\n",
    "    't': 'auto',\n",
    "    'n_landmark': 5000,\n",
    "    'verbose': False\n",
    "}\n",
    "training_hypers = {\n",
    "    'data_name': f'{data_name}',\n",
    "    'mode': mode, # 'encoder', 'decoder', 'end2end', 'separate\n",
    "    'max_epochs': 100,\n",
    "    'batch_size': 64,\n",
    "    'lr': 1e-3,\n",
    "    'shuffle': True,\n",
    "    'componentwise_std': False,\n",
    "    'weight_decay': 1e-5,\n",
    "    'dist_mse_decay': 1e-5,\n",
    "    'monitor': 'validation/loss',\n",
    "    'patience': 100,\n",
    "    'seed': 2024,\n",
    "    'log_every_n_steps': 100,\n",
    "    'accelerator': 'auto',\n",
    "    'train_from_scratch': True,\n",
    "    'model_save_path': f'./{data_name}_distance_matching_{mode}/model'\n",
    "}\n",
    "\n",
    "model = DistanceMatching(**model_hypers)\n",
    "model.fit(X, train_mask=None, percent_test=0.2, **training_hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model.encode(torch.Tensor(X))\n",
    "print('Encoded Z:', Z.shape)\n",
    "X_hat = model.decode(Z)\n",
    "print('Decoded X:', X_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig = plt.figure(figsize=(20, 6))\n",
    "ax = fig.add_subplot(131, projection='3d')\n",
    "scprep.plot.scatter3d(X_hat.detach().numpy(), c='b', title='Reconstructed', ax=ax)\n",
    "\n",
    "if Z.shape[-1] < 3:\n",
    "    ax = fig.add_subplot(132)\n",
    "    scprep.plot.scatter2d(Z.detach().numpy(), c='b', title='Latent', ax=ax)\n",
    "else:\n",
    "    ax = fig.add_subplot(132, projection='3d')\n",
    "    scprep.plot.scatter3d(Z.detach().numpy(), c='b', title='Latent', ax=ax)\n",
    "\n",
    "ax = fig.add_subplot(133, projection='3d')\n",
    "scprep.plot.scatter3d(X, c='b', title='Original', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = model.encoder_pullback(torch.Tensor(X))\n",
    "print('Encoder Pullback:', metric.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly pick a pair of points in ambient space\n",
    "def random_pair(Z, num_endpoints=32):\n",
    "    if isinstance(Z, torch.Tensor):\n",
    "        Z = Z.detach().numpy()\n",
    "    z_end = Z[np.random.randint(0, Z.shape[0]), :]\n",
    "    z_start = Z[np.random.randint(0, Z.shape[0], num_endpoints), :]\n",
    "    print('z_start:', z_start.shape)\n",
    "    print('z_end:', z_end.shape)\n",
    "\n",
    "    batch_x0 = torch.Tensor(z_start)\n",
    "    batch_x1 = torch.Tensor(np.repeat(z_end.reshape(1, -1), num_endpoints, axis=0))\n",
    "\n",
    "    print('batch_x0:', batch_x0.shape)\n",
    "    print('batch_x1:', batch_x1.shape)\n",
    "\n",
    "    return z_start, z_end, batch_x0, batch_x1\n",
    "\n",
    "# Z = Z.detach().cpu().numpy()\n",
    "# X = X.detach().cpu().numpy()\n",
    "z_start, z_end, batch_x0, batch_x1 = random_pair(X, num_endpoints=32)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(batch_x0, batch_x1) # same end points, different start points for NeuralODE\n",
    "#dataset = torch.utils.data.TensorDataset(batch_x1, batch_x0) \n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Visualize x0, x1 in latent space\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "if z_start.shape[1] < 3:\n",
    "    ax = fig.add_subplot(111)\n",
    "    scprep.plot.scatter2d(Z, c='gray', title='Latent', ax=ax)\n",
    "    scprep.plot.scatter2d(z_start, c='red', ax=ax)\n",
    "    scprep.plot.scatter2d(z_end.reshape(1, -1), c='blue', ax=ax)\n",
    "else:\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scprep.plot.scatter3d(X, c='gray', title='Latent', ax=ax)\n",
    "    scprep.plot.scatter3d(z_start, c='red', ax=ax)\n",
    "    scprep.plot.scatter3d(z_end.reshape(1, -1), c='blue', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Density Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjoint = False\n",
    "if adjoint:\n",
    "    from torchdiffeq import odeint_adjoint as odeint\n",
    "else:\n",
    "    from torchdiffeq import odeint\n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "    \"\"\"\n",
    "    For simplicity we are just using 2 layers but it might worth to substitute with the MLP class\n",
    "    although the torchdiffeq suggusted using tanh activation which we might want to tune.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, self.hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_dim, self.in_dim),\n",
    "        )\n",
    "    def forward(self, t, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odefunc = ODEFunc(3, 32)\n",
    "ts = torch.linspace(0, 1, 100)\n",
    "path = odeint(odefunc, batch_x1, ts) # [T, B, D]\n",
    "print(path.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoODE with Density Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "def compute_jacobian_function(f, x, create_graph=True, retain_graph=True):\n",
    "    \"\"\"\n",
    "    Compute the Jacobian of function f wrt x using an efficient broadcasting approach.\n",
    "    Args:\n",
    "        f: The function to compute the Jacobian of. f: (B, D) -> (B, n).\n",
    "        x: (B, D) A batch of points in the dim D.\n",
    "    Returns:\n",
    "        jacobian: (B, n, D) The Jacobian of f wrt x.\n",
    "    \"\"\"\n",
    "    # z_batch = z_batch.clone().detach().requires_grad_(True)\n",
    "    x = x.clone()\n",
    "    x.requires_grad_(True)\n",
    "    # model.no_grad()\n",
    "    output = f(x)\n",
    "    batch_size, output_dim, input_dim = *output.shape, x.shape[-1]\n",
    "\n",
    "    # Use autograd's grad function to get gradients for each output dimension\n",
    "    jacobian = torch.zeros(batch_size, output_dim, input_dim).to(x.device)\n",
    "    for i in range(output_dim):\n",
    "        grad_outputs = torch.zeros(batch_size, output_dim).to(x.device)\n",
    "        grad_outputs[:, i] = 1.0\n",
    "        gradients = grad(outputs=output, inputs=x, grad_outputs=grad_outputs, create_graph=create_graph, retain_graph=retain_graph, only_inputs=True)[0]\n",
    "        print(gradients.shape)\n",
    "        print(gradients.mean())\n",
    "        jacobian[:, i, :] = gradients\n",
    "    return jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac = compute_jacobian_function(model.encoder, torch.rand(10, 3))\n",
    "print(jac.shape)\n",
    "print(model.encoder(torch.rand(10, 3)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "    \n",
    "class GeodesicODEDensity(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "        fcn, # encoder/decoder\n",
    "        in_dim=3, \n",
    "        hidden_dim=32, \n",
    "        n_tsteps=1000, # num of t steps for length evaluation\n",
    "        lam=10, # regularization for end point\n",
    "        lr=1e-3, \n",
    "        weight_decay=1e-5, \n",
    "        beta=0.,\n",
    "        n_pow=4,\n",
    "        data_pts=None,\n",
    "        n_data_sample=None,\n",
    "        n_topk=5,\n",
    "        density_weight=1.,\n",
    "    ):\n",
    "        #super().__init__(fcn, in_dim, hidden_dim, n_tsteps, lam, lr, weight_decay, beta, n_pow)\n",
    "        super().__init__()\n",
    "        # self.save_hyperparameters()\n",
    "        \n",
    "        self.odefunc = ODEFunc(in_dim, hidden_dim)\n",
    "        \n",
    "        self.pretraining = False\n",
    "        self.t = torch.linspace(0, 1, n_tsteps)\n",
    "        self.register_buffer(\"data_pts\", data_pts)\n",
    "        self.n_data_sample = n_data_sample\n",
    "        self.n_topk = n_topk\n",
    "        self.density_weight = density_weight\n",
    "        self.fcn = fcn\n",
    "\n",
    "        self.lam = lam\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.beta = beta\n",
    "        self.n_pow = n_pow\n",
    "\n",
    "        # freeze fcn\n",
    "        fcn.requires_grad_(False)\n",
    "\n",
    "    \n",
    "    def forward(self, x0):\n",
    "        '''\n",
    "            x0: [B, D]\n",
    "            xt: [T, B, D]\n",
    "        '''\n",
    "        t = self.t\n",
    "        x_t = odeint(self.odefunc, x0, t)\n",
    "        print(x_t.shape)\n",
    "        return x_t\n",
    "\n",
    "    def density_loss(self, x_t_flat, data_pts):\n",
    "        vals, inds = torch.topk(\n",
    "            torch.cdist(x_t_flat, data_pts), k=self.n_topk, dim=-1, largest=False, sorted=False\n",
    "        )\n",
    "        return vals.mean()\n",
    "\n",
    "    def length_loss(self, t, x):\n",
    "        '''\n",
    "            t: [T]\n",
    "            x: [T, B, D]\n",
    "        '''\n",
    "        print('x.shape:', x.shape)\n",
    "        x_flat = x.view(-1, x.shape[2]) # [T*B, D]\n",
    "        print(x_flat.mean())\n",
    "        jac = compute_jacobian_function(self.fcn, x_flat) # [T*B, n, D]\n",
    "        print('jac.shape', jac.shape)\n",
    "        #print(jac)\n",
    "        print(jac.mean())\n",
    "        metric_flat = torch.einsum('nij,nik->njk', jac, jac) # [T*B, D, D]\n",
    "        # metric_flat = pullback_metric(x_flat, self.hparams.fcn, create_graph=True, retain_graph=True)\n",
    "        \n",
    "        xdot = self.odefunc(t, x) # [T, B, D], the velocity\n",
    "        xdot_flat = xdot.view(-1, xdot.shape[2]) # [T*B, D]\n",
    "        #print('metric_flat:', metric_flat.shape)\n",
    "        print(metric_flat.mean())\n",
    "        l_flat = torch.sqrt(torch.einsum('Ni,Nij,Nj->N', xdot_flat, metric_flat, xdot_flat)) # [T*B]\n",
    "\n",
    "        print('l_flat:', l_flat.shape)\n",
    "        print(l_flat.mean())\n",
    "        \n",
    "        return l_flat.mean() # * (t[-1] - t[0]) # numerical integration, we set t in [0,1].\n",
    "\n",
    "    def step(self, batch, batch_idx):\n",
    "        t = self.t\n",
    "        x0, x1 = batch #[B, D]\n",
    "        x_t = self.forward(x0)  \n",
    "\n",
    "        mse_loss = F.mse_loss(x_t[-1], x1) # endpoint loss\n",
    "        if self.pretraining:\n",
    "            return mse_loss\n",
    "        mpowerede_loss = 0.\n",
    "        if self.beta > 0.:\n",
    "            mpowerede_loss = (torch.pow(x_t[-1] - x1, self.n_pow)).mean() * self.beta\n",
    "        \n",
    "        len_loss = self.length_loss(t, x_t)\n",
    "        loss = len_loss + self.lam * mse_loss + mpowerede_loss\n",
    "        \n",
    "        if self.density_weight > 0.:\n",
    "            x_t_flat = x_t.view(-1, x_t.shape[2])\n",
    "            if self.n_data_sample is not None and self.n_data_sample < self.data_pts.size(0):\n",
    "                indices = torch.randperm(self.data_pts.size(0))[:self.n_data_sample]\n",
    "                dloss = self.density_loss(x_t_flat, self.data_pts[indices])\n",
    "            else:\n",
    "                dloss = self.density_loss(x_t_flat, self.data_pts)\n",
    "            loss += self.density_weight * dloss\n",
    "        print('len_loss:', len_loss, 'endpoint_loss:', self.lam * mse_loss, 'density_l:', dloss)\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.step(batch, batch_idx)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.step(batch, batch_idx)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.step(batch, batch_idx)\n",
    "        self.log('test_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_lambda = 10.0\n",
    "density_lambda = 100.0\n",
    "\n",
    "geo_ode = GeodesicODEDensity(model.encoder, in_dim=X.shape[-1], hidden_dim=64, n_tsteps=100, \n",
    "                             lam=endpoint_lambda, lr=1e-3, weight_decay=0, beta=0., n_pow=4, \n",
    "                             data_pts=torch.Tensor(X), n_data_sample=None, n_topk=5, density_weight=density_lambda)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=300, gpus=0)\n",
    "trainer.fit(geo_ode, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "n_samples = 1\n",
    "t = torch.linspace(0, 1, 1000).view(-1,1)\n",
    "pred_geodesic = geo_ode(batch_x0[:, :]).detach().numpy()\n",
    "print('Pred Geodesic:', pred_geodesic.shape)\n",
    "print(pred_geodesic[:10, :n_samples, :].shape)\n",
    "print((pred_geodesic[:10, :n_samples, :]).reshape(-1,3))\n",
    "print((pred_geodesic[:-10, :n_samples, :]).reshape(-1,3))\n",
    "print('endpoint: ', batch_x0[:n_samples, :])\n",
    "print('startpoint: ', batch_x1[:n_samples, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "# Visualize pred geodesic on the ambient space\n",
    "if pred_geodesic.shape[-1] < 3:\n",
    "    ax = fig.add_subplot(111)\n",
    "    scprep.plot.scatter2d(X, c='gray', ax=ax)\n",
    "    scprep.plot.scatter2d(batch_x0.detach().numpy(), c='g', ax=ax, alpha=0.5)\n",
    "    scprep.plot.scatter2d(batch_x1.detach().numpy(), c='r', ax=ax)\n",
    "    for i in range(pred_geodesic.shape[1]):\n",
    "        scprep.plot.scatter2d(pred_geodesic[:, i, :].reshape(1, -1), ax=ax)\n",
    "else:\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scprep.plot.scatter3d(X, c='gray', ax=ax, alpha=0.5)\n",
    "    scprep.plot.scatter3d(batch_x0.detach().numpy()[:,:], c='g', ax=ax)\n",
    "    scprep.plot.scatter3d(batch_x1.detach().numpy()[:,:], c='r', ax=ax)\n",
    "    for i in range(pred_geodesic.shape[1]):\n",
    "        scprep.plot.scatter3d(pred_geodesic[:, i, :].reshape(1, -1), ax=ax, c='b')\n",
    "\n",
    "    # anmi = scprep.plot.rotate_scatter3d(X, c='gray', ax=ax, alpha=0.5)\n",
    "    # anmi = scprep.plot.rotate_scatter3d(batch_x0.detach().numpy()[:n_samples,:], c='g', ax=ax)\n",
    "    # anmi = scprep.plot.rotate_scatter3d(batch_x1.detach().numpy()[:n_samples,:], c='r', ax=ax)\n",
    "    # anmi = scprep.plot.rotate_scatter3d(pred_geodesic, c='b', title='Pred Geodesic', ax=ax)\n",
    "\n",
    "    # anmi.save(f'density{density_lambda}_{data_name}_geoODE.gif', writer='imagemagick', fps=60)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly 3D scatter plot\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode()\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "n_samples = 20\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter3d(x=X[:,0], y=X[:,1], z=X[:,2], mode='markers', marker=dict(size=5, color='gray', opacity=0.5)))\n",
    "fig.add_trace(go.Scatter3d(x=batch_x0[:n_samples,0], y=batch_x0[:,1], z=batch_x0[:,2], mode='markers', marker=dict(size=5, color='red')))\n",
    "fig.add_trace(go.Scatter3d(x=batch_x1[:n_samples,0], y=batch_x1[:,1], z=batch_x1[:,2], mode='markers', marker=dict(size=5, color='green')))\n",
    "for i in range(n_samples):\n",
    "    fig.add_trace(go.Scatter3d(x=pred_geodesic[:,i,0], y=pred_geodesic[:,i,1], z=pred_geodesic[:,i,2], mode='markers', marker=dict(size=5, color='blue')))\n",
    "\n",
    "# save\n",
    "plotly.offline.plot(fig, filename=f'density{density_lambda}_{data_name}_geoODE.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yale529",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
